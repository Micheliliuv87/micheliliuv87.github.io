<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Distance Measures for Data Science | Qirui(Micheli) Liu </title> <meta name="author" content="Qirui(Micheli) Liu"> <meta name="description" content="Include many distance measures: which come in handy and help me through many of my data science projects"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/MickVicon.jpeg?4228235fca6310e5e9d7247ffa5842ef"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://micheliliuv87.github.io/blog/2025/distance-measure/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Distance Measures for Data Science",
            "description": "Include many distance measures: which come in handy and help me through many of my data science projects",
            "published": "October 28, 2025",
            "authors": [
              
              {
                "author": "Qirui(Micheli) Liu",
                "authorURL": "https://micheliliuv87.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Qirui(Micheli)</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Distance Measures for Data Science</h1> <p>Include many distance measures: which come in handy and help me through many of my data science projects</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#cosine-type-distance">Cosine Type Distance</a> </div> <ul> <li> <a href="#cosine-distance">Cosine Distance</a> </li> <li> <a href="#pearson-correlation-distance">Pearson Correlation Distance</a> </li> <li> <a href="#spearman-correlation-distance-rank-corr">Spearman Correlation Distance (Rank Corr)</a> </li> </ul> <div> <a href="#generalized-distance-metrics">Generalized Distance (Metrics)</a> </div> <ul> <li> <a href="#minkowski-distance">Minkowski Distance</a> </li> <li> <a href="#euclidean-distance">Euclidean Distance</a> </li> <li> <a href="#chebyshev-distance-max-diff-in-coordinate-points">Chebyshev Distance (Max diff. in Coordinate points)</a> </li> </ul> <div> <a href="#scaled-weighted-distance">Scaled Weighted Distance</a> </div> <ul> <li> <a href="#manhalanobis-distance-scaled-euclidean">Manhalanobis Distance (Scaled Euclidean)</a> </li> <li> <a href="#canberra-distance-weighted-manhattan">Canberra Distance (Weighted Manhattan)</a> </li> </ul> </nav> </d-contents> <h3 id="notes-for-readers"><strong>Notes for Readers:</strong></h3> <ol> <li> <p><strong>Foundational for Data Science:</strong> These distance measures are the bedrock of many fundamental algorithms. You will use them in:</p> <ul> <li> <strong>Clustering</strong> (e.g., K-Means, Hierarchical Clustering)</li> <li> <strong>Classification</strong> (e.g., K-Nearest Neighbors)</li> <li> <strong>Anomaly Detection</strong> (to find outliers)</li> <li> <strong>Recommendation Systems</strong> (to find similar users or items)</li> <li> <strong>Dimensionality Reduction</strong> (e.g., in the core of MDS or t-SNE)</li> </ul> </li> <li> <p><strong>Ubiquitous in Practice:</strong> The distances covered here include notes and hands-on practices you can try on your own. You will likely encounter them repeatedly in coursework, projects, and real-world applications.</p> </li> <li> <p><strong>A Starting Point, Not the Finish Line:</strong> This collection is not exhaustive (it omits, for example, Hamming distance, Jaccard index, and Earth Mover’s Distance), but it covers the most distances that are at least used once or multiple times in my school and work projects. So it is a good starting point to begin and learn.</p> </li> </ol> <h1 id="cosine-type-distance"><strong>Cosine Type Distance</strong></h1> <h2 id="cosine-distance-almost-same-direction-in-high-dimentional-vectors-and-similar-angle"><strong>Cosine Distance (Almost same direction in high dimentional vectors, and similar angle)</strong></h2> <p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20200911171455/UntitledDiagram2.png" alt="Cosine Similarity" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"></p> <h4 id="definition"><strong>Definition</strong></h4> <p>For vectors $\mathbf{s}$ and $\mathbf{t}$ in d-dimensional space, the <strong>cosine similarity</strong> is defined as:</p> \[\cos(\mathbf{s}, \mathbf{t}) = \frac{\mathbf{s}^\mathsf{T}\mathbf{t}}{\|\mathbf{s}\|_2 \, \|\mathbf{t}\|_2} = \frac{\sum_{j=1}^{d} s_j t_j}{\sqrt{\sum_{j=1}^{d} s_j^2} \cdot \sqrt{\sum_{j=1}^{d} t_j^2}}\] <p>Then the <strong>cosine distance</strong> is:</p> \[d(\mathbf{s}, \mathbf{t}) = 1 - \cos(\mathbf{s}, \mathbf{t})\] <ul> <li>Value Range: Cosine similarity ranges between $[-1, +1]$ <strong>[−1 (perfectly dissimilar) and +1 (perfectly similar)]</strong>. If two vectors have the same direction, similarity $= +1$, distance $= 0$; if they have opposite directions, similarity $= -1$, distance $= 2$(though -1 is less common in many real-world datasets if all values are non-negative).</li> </ul> <h4 id="why-and-how-its-used"><strong>Why and How It’s Used</strong></h4> <ol> <li>Focuses on Orientation, Not Magnitude <ul> <li>Cosine measures the angle between vectors, independent of their magnitudes.</li> <li>Even if vectors differ in overall scale, the cosine similarity can remain high if they point in a similar direction.</li> </ul> </li> <li>Common in Text Analysis <ul> <li>In high-dimensional sparse vectors like (Bag-of-words) TF–IDF, cosine similarity is high if the directions are similar, regardless of absolute frequencies.</li> </ul> </li> <li>Computationally Simple <ul> <li>Only requires the dot product and vector norms, making it relatively efficient to compute.</li> </ul> </li> </ol> <h4 id="how-it-works-intuitive-understanding"><strong>How It Works (Intuitive Understanding)</strong></h4> <ul> <li>The more similar the directions of two vectors, the smaller the angle → the greater the cosine similarity → the smaller the cosine distance.</li> <li>If vectors are almost orthogonal or diverge, the cosine similarity is small → the cosine distance is large.</li> </ul> <h4 id="interpretation"><strong>Interpretation</strong></h4> <ul> <li>$vec_b$ is essentially $vec_a * 2$, so they point in the exact same direction.</li> <li>Cosine similarity is 1.0 → Cosine distance is 0.</li> </ul> <h4 id="example"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">cosine_distance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="n">dot_product</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">norm_s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="c1"># np.dot: try to write this out in your own practice because dot product is already vectorized.
</span>    <span class="n">norm_t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="c1"># Cosine similarity
</span>    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">dot_product</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm_s</span> <span class="o">*</span> <span class="n">norm_t</span><span class="p">)</span>

    <span class="c1"># Cosine distance
</span>    <span class="n">cos_dist</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cos_sim</span>
    <span class="k">return</span> <span class="n">cos_dist</span>

<span class="c1"># Example usage:
</span><span class="n">vec_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">vec_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>

<span class="n">dist</span> <span class="o">=</span> <span class="nf">cosine_distance</span><span class="p">(</span><span class="n">vec_a</span><span class="p">,</span> <span class="n">vec_b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Cosine Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Cosine Distance: 2.0
</code></pre></div></div> <h4 id="in-more-detail"><strong>In More Detail</strong></h4> <p><strong>Dot Product Part</strong> ($\mathbf{s}^\mathsf{T}\mathbf{t}$ is exactly the <strong>dot product</strong>: $\mathbf{s}^\mathsf{T}\mathbf{t} = \sum_{j=1}^{d} s_j t_j$)</p> <ul> <li>In English, “$\mathbf{s}^\mathsf{T}\mathbf{t}$” is exactly the dot product of vectors $\mathbf{s}$ and $\mathbf{t}$.</li> <li>In this formula, $\mathbf{s}^\mathsf{T}\mathbf{t}$ represents the sum of the coordinate-wise products of vectors $\mathbf{s}$ and $\mathbf{t}$, which is exactly what we call the vector dot product.</li> </ul> <p><strong>Vector Norm</strong> (This $|\mathbf{s}|_2$ and $|\mathbf{t}|_2$ are the <strong>Vector Norms</strong>)</p> <ul> <li>$|\mathbf{s}|_2$ is also called the L2 norm (Euclidean norm) of vector $\mathbf{s}$, which can be understood as the vector’s length or magnitude.</li> <li>Mathematical definition: $|\mathbf{s}|<em>2 = \sqrt{\sum</em>{j=1}^{d} s_j^2}$.</li> <li>$|\mathbf{s}|_2$ represents the length of vector $\mathbf{s}$, calculated by squaring each coordinate, summing these squares, and then taking the square root.</li> </ul> <h4 id="example-1"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>

<span class="c1"># dot product
</span><span class="n">dot_value</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">s • t (dot product):</span><span class="sh">"</span><span class="p">,</span> <span class="n">dot_value</span><span class="p">)</span>

<span class="c1"># norm
</span><span class="n">norm_s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">norm_t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">||s||_2:</span><span class="sh">"</span><span class="p">,</span> <span class="n">norm_s</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">||t||_2:</span><span class="sh">"</span><span class="p">,</span> <span class="n">norm_t</span><span class="p">)</span>

<span class="c1"># cosine similarity
</span><span class="n">cos_sim</span> <span class="o">=</span> <span class="n">dot_value</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm_s</span> <span class="o">*</span> <span class="n">norm_t</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Cosine similarity:</span><span class="sh">"</span><span class="p">,</span> <span class="n">cos_sim</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>s • t (dot product): 32
||s||<span class="se">\_</span>2: 3.7416573867739413
||t||<span class="se">\_</span>2: 8.774964387392123
Cosine similarity: 0.9746318461970762
</code></pre></div></div> <hr> <p><br></p> <h2 id="pearson-correlation-distance"><strong>Pearson Correlation Distance</strong></h2> <p>(The first array appears to be derived by subtracting a constant from the second array)</p> <p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20250723175534566635/pearson_correlation_coefficient.webp" alt="Pearson Correlation" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"></p> <h4 id="definition-1"><strong>Definition</strong></h4> <ul> <li>The Pearson correlation coefficient measures the <strong>linear</strong> relationship between two vectors.</li> <li>Sentence Structure: We first perform mean-centering on vectors $\mathbf{x}$ and $\mathbf{y}$, then compute their cosine similarity.</li> <li>English Terms: mean-centering, cosine similarity</li> </ul> <h4 id="mathematical-expression"><strong>Mathematical Expression</strong></h4> \[\mathrm{corr}(\mathbf{x}, \mathbf{y}) = \frac{\sum_{j=1}^{d} (x_j - \bar{x})(y_j - \bar{y})} {\sqrt{\sum_{j=1}^{d} (x_j - \bar{x})^2}\;\sqrt{\sum_{j=1}^{d} (y_j - \bar{y})^2}}\] <p>Where \(\bar{x} = \frac{1}{d}\sum_{j=1}^{d} x_j \text{ --- is the mean of } \mathbf{x}\) \(\bar{y} = \frac{1}{d}\sum_{j=1}^{d} y_j \text{ --- is the mean of } \mathbf{y}\)</p> <h4 id="pearson-correlation-distance-is-defined-as"> <strong>Pearson Correlation Distance is defined as</strong>:</h4> \[d_{\text{Pearson}}(\mathbf{x}, \mathbf{y}) = 1 - \mathrm{corr}(\mathbf{x}, \mathbf{y})\] <h4 id="why-and-how-its-used-1"><strong>Why and How It’s Used</strong></h4> <ol> <li> <strong>Measures Linear Association</strong> <ul> <li>Sentence Structure: When you want to know if two vectors have a proportional or inversely proportional linear relationship, you can use the Pearson correlation coefficient. <ul> <li>A smaller distance (correlation coefficient closer to 1) indicates higher similarity in linear variation across dimensions.</li> </ul> </li> <li>English Terms: linear relationship</li> </ul> </li> <li> <strong>Insensitive to Scale and Offset</strong> <ul> <li>If one vector is just a linear scaling or shift of another, they can still have a high Pearson correlation coefficient.</li> </ul> </li> <li> <strong>Applications in Data Analysis</strong> <ul> <li>Feature selection: Find features highly linearly associated with the target variable.</li> <li>Clustering: Group samples with similar linear variation patterns.</li> </ul> </li> </ol> <h4 id="how-it-works"><strong>How It Works</strong></h4> <ul> <li>First, subtract the mean from each vector (mean-centering).</li> <li>Then perform a calculation similar to cosine similarity, but using the “mean-centered” vectors.</li> <li>Finally, use <code class="language-plaintext highlighter-rouge">1 - corr</code> to get the distance value: higher correlation coefficient ⇒ smaller distance.</li> </ul> <h4 id="example-2"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">pearson_correlation_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># change to float better for calculation
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

    <span class="c1"># calculate mean
</span>    <span class="n">x_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># mean-centering
</span>    <span class="n">x_centered</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x_mean</span>
    <span class="n">y_centered</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_mean</span>

    <span class="c1"># dot product
</span>    <span class="n">dot_xy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x_centered</span><span class="p">,</span> <span class="n">y_centered</span><span class="p">)</span>

    <span class="c1"># norm
</span>    <span class="n">norm_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x_centered</span><span class="p">)</span>
    <span class="n">norm_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">y_centered</span><span class="p">)</span>

    <span class="c1"># If one vector is all constant(except prime numbers), norm will be 0, thus the correlation distance can be set to 1
</span>    <span class="k">if</span> <span class="n">norm_x</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">norm_y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1.0</span>

    <span class="c1"># Pearson correlation
</span>    <span class="n">pearson_corr</span> <span class="o">=</span> <span class="n">dot_xy</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm_x</span> <span class="o">*</span> <span class="n">norm_y</span><span class="p">)</span>

    <span class="c1"># Pearson distance
</span>    <span class="n">distance</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">pearson_corr</span>
    <span class="k">return</span> <span class="n">distance</span>

<span class="c1"># Sample data
</span><span class="n">vec_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>
<span class="n">vec_b</span> <span class="o">=</span> <span class="p">[</span> <span class="mi">5</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="n">dist</span> <span class="o">=</span> <span class="nf">pearson_correlation_distance</span><span class="p">(</span><span class="n">vec_a</span><span class="p">,</span> <span class="n">vec_b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Pearson Correlation Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist</span><span class="p">)</span>

<span class="c1"># Note: in this case, vec_b appears to be vec_a minus one constant (offsey ~5). If we are expecting they have higher correlation, the distance will be smaller.
</span></code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Pearson Correlation Distance: -2.220446049250313e-16
</code></pre></div></div> <hr> <p><br></p> <h2 id="spearman-correlation-distance-rank-correlation"><strong>Spearman Correlation Distance (Rank Correlation)</strong></h2> <p><img src="https://statistics.laerd.com/statistical-guides/img/spearman-1-small.png" alt="Spearman Correlation" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"></p> <h4 id="definition-2"><strong>Definition</strong></h4> <ul> <li>The Spearman correlation coefficient measures the <strong>rank-based</strong> monotonic relationship between two vectors.</li> <li>Sentence Structure: First map the element values of each vector to their respective ranks within the vector, then perform operations similar to Pearson correlation on these “rank vectors.”</li> </ul> <h4 id="mathematical-expression-1"><strong>Mathematical Expression</strong></h4> <ul> <li>Similar to <a href="#pearson-correlation-distance-the-first-array-appears-to-be-derived-by-subtracting-a-constant-from-the-second-array">Pearson</a>, but instead of mean-centering vectors $\mathbf{x}$ and $\mathbf{y}$, first convert $\mathbf{x}$ to its rank vector, then convert $\mathbf{y}$ to its rank vector, and then calculate the Pearson correlation coefficient.</li> </ul> \[\mathrm{corr}_{\mathrm{Spearman}}(\mathbf{x}, \mathbf{y}) = \mathrm{corr}(\mathrm{rank}(\mathbf{x}),\, \mathrm{rank}(\mathbf{y}))\] <ul> <li>The corresponding <strong>Spearman Correlation Distance</strong> is then:</li> </ul> \[d_{\text{Spearman}}(\mathbf{x}, \mathbf{y}) = 1 - \mathrm{corr}_{\mathrm{Spearman}}(\mathbf{x}, \mathbf{y})\] <h4 id="why-and-how-its-used-2"><strong>Why and How It’s Used</strong></h4> <ol> <li> <strong>Focuses on Ranking Rather Than Specific Values</strong> <ul> <li>Sentence Structure: As long as the order of values in two vectors is consistent (or nearly consistent), even if their value distributions aren’t strictly linear, they can still achieve a high Spearman correlation.</li> <li>English Terms: rank, monotonic</li> </ul> </li> <li> <strong>More Robust to Outliers</strong> <ul> <li>Because it only considers relative size (which is larger/smaller) and not the absolute magnitude of differences, extreme points don’t affect it as much as they do in Pearson correlation.</li> </ul> </li> <li> <strong>Used in Non-linear Monotonic Relationship Scenarios</strong> <ul> <li>For example, when one vector increases as the other increases (possibly in a curved relationship), Spearman can still capture this monotonic trend.</li> </ul> </li> </ol> <h4 id="how-it-works-1"><strong>How It Works</strong></h4> <ul> <li>Rank the elements of $\mathbf{x}$ and $\mathbf{y}$ from smallest to largest.</li> <li>After obtaining the rank vectors, compute the Pearson correlation, then take 1 - correlation to get the distance.</li> </ul> <h4 id="example-3"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">spearman_correlation_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

    <span class="c1"># make ranks for x,y  (rank)
</span>    <span class="n">x_rank</span> <span class="o">=</span> <span class="nf">rank_vector</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y_rank</span> <span class="o">=</span> <span class="nf">rank_vector</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># calculate pearson correlation but with x_rank, y_rank
</span>    <span class="k">return</span> <span class="nf">pearson_correlation_distance</span><span class="p">(</span><span class="n">x_rank</span><span class="p">,</span> <span class="n">y_rank</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">rank_vector</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="c1"># order ascending by rank and give min rank=1，2nd min rank=2，and so on
</span>    <span class="n">sorted_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="n">ranks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">sorted_idx</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ranks</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
    <span class="k">return</span> <span class="n">ranks</span>

<span class="k">def</span> <span class="nf">pearson_correlation_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># use the same Pearson equation
</span>    <span class="n">x_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">x_centered</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x_mean</span>
    <span class="n">y_centered</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_mean</span>

    <span class="n">dot_xy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x_centered</span><span class="p">,</span> <span class="n">y_centered</span><span class="p">)</span>
    <span class="n">norm_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x_centered</span><span class="p">)</span>
    <span class="n">norm_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">y_centered</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">norm_x</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">norm_y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1.0</span>
    <span class="n">corr</span> <span class="o">=</span> <span class="n">dot_xy</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm_x</span> <span class="o">*</span> <span class="n">norm_y</span><span class="p">)</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">corr</span>
    <span class="k">return</span> <span class="n">distance</span>

<span class="c1"># sample data
</span><span class="n">vec_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
<span class="n">vec_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="n">dist_spearman</span> <span class="o">=</span> <span class="nf">spearman_correlation_distance</span><span class="p">(</span><span class="n">vec_a</span><span class="p">,</span> <span class="n">vec_b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Spearman Correlation Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_spearman</span><span class="p">)</span>

<span class="c1">#	in vec_a, order is (10, 20, 100) When they rank，you get rank = (1, 2, 3).
#	in vec_b, order is (5, 10, 90) when they rank, you get rank = (1, 2, 3).
#	Therefore their rank is basically the same, so Spearman distance will be low
</span></code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Spearman Correlation Distance: 2.220446049250313e-16
</code></pre></div></div> <hr> <p><br></p> <p><br></p> <h1 id="gernalized-distance-metric"><strong>Gernalized Distance (Metric)</strong></h1> <h2 id="minkowski-distance"><strong>Minkowski Distance</strong></h2> <p><img src="https://www.kdnuggets.com/wp-content/uploads/c_distance_metrics_euclidean_manhattan_minkowski_oh_12.jpg" alt="Minkowski Distance" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"></p> <h4 id="definition-3"><strong>Definition</strong></h4> <ul> <li>For two points $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ and $\mathbf{y} = (y_1, y_2, \ldots, y_n)$ in n-dimensional space, the Minkowski distance is defined as:</li> </ul> \[d(\mathbf{x}, \mathbf{y}) = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{\frac{1}{p}}\] <h4 id="where-p-is-a-parameter-that-determines-the-type-of-distance"><strong>Where $p$ is a parameter that determines the type of distance:</strong></h4> <ul> <li>When $p = 1$, it yields the Manhattan Distance.</li> <li>When $p = 2$, it yields the Euclidean Distance.</li> <li>As long as $p \ge 1$, it is a valid Minkowski distance.</li> </ul> <h4 id="why-and-how-its-used-3"><strong>Why and How It’s Used</strong></h4> <ol> <li> <em>Generalization</em>: Minkowski distance is a generalized form of many other distance metrics (like Euclidean, Manhattan).</li> <li> <em>Flexibility</em>: You can choose different $p$ values based on requirements: <ul> <li>$p = 1$: Scenarios that emphasize absolute differences (like city block distance).</li> <li>$p = 2$: The most commonly used straight-line distance.</li> </ul> </li> <li> <em>Machine Learning</em>: In algorithms like k-Nearest Neighbors (k-NN), you can try different $p$ values to see which distance metric performs better.</li> </ol> <h4 id="how-it-works-intuitive-understanding-1"><strong>How It Works (Intuitive Understanding)</strong></h4> <ul> <li>First calculate the absolute difference for each coordinate, raise it to the power of $p$, sum these values, and finally take the <strong>p-th root</strong>.</li> <li>When $p$ increases, larger differences have a greater impact on the result.</li> </ul> <h4 id="example-4"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">minkowski_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="c1"># x and y are lists or NumPy arrays
</span>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="n">p</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">p</span><span class="p">)</span>

<span class="c1"># Example usage:
</span><span class="n">point_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">point_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">dist_p1</span> <span class="o">=</span> <span class="nf">minkowski_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Manhattan Distance
</span><span class="n">dist_p2</span> <span class="o">=</span> <span class="nf">minkowski_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Euclidean Distance
</span><span class="n">dist_p3</span> <span class="o">=</span> <span class="nf">minkowski_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Minkowski Distance with p=1 (Manhattan):</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_p1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Minkowski Distance with p=2 (Euclidean):</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_p2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Minkowski Distance with p=3:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_p3</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Minkowski Distance with p=1 (Manhattan): 9.0
Minkowski Distance with p=2 (Euclidean): 5.385164807134504
Minkowski Distance with p=3: 4.626065009182741
</code></pre></div></div> <hr> <p><br></p> <h2 id="euclidean-distance"><strong>Euclidean Distance</strong></h2> <p>(Point it out, use very often in your data science projects)</p> <p><img src="https://rosalind.info/media/Euclidean_distance.png" alt="Euclidean Distance" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"></p> <h4 id="definition-4"><strong>Definition</strong></h4> <ul> <li>For two points $\mathbf{x}$ and $\mathbf{y}$ in n-dimensional space, the Euclidean distance (also called L2 distance) is defined as:</li> </ul> \[d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}\] <p>It is a special case of the Minkowski distance when $p = 2$.</p> <h4 id="why-and-how-its-used-4">Why and How It’s Used</h4> <ol> <li> <strong>Straight-line Distance</strong>: It represents the “straight-line” distance between points in Euclidean space.</li> <li> <strong>Usage in Machine Learning</strong>: <ul> <li>In k-Nearest Neighbors (k-NN), it is used to measure similarity or dissimilarity between samples.</li> <li>In clustering (such as k-means), Euclidean distance is often used as a measure of cluster compactness.</li> </ul> </li> <li> <strong>Geometric Meaning</strong>: It has a very clear geometric interpretation - the length of the line segment connecting the two points.</li> </ol> <h4 id="how-it-works-intuitive-understanding-2">How It Works (Intuitive Understanding)</h4> <ul> <li>Calculate the difference for each corresponding coordinate, square it, sum up all the squared differences, and then take the square root.</li> <li>If the differences are large, the distance will be large as well.</li> </ul> <h4 id="example-5"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span> <span class="c1">#this 1/2 can use sqrt to replace
</span>    <span class="c1">#return np.sum((x - y) ** 2) ** (1/2)
</span>
<span class="n">point_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">point_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">dist_euclidean</span> <span class="o">=</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Euclidean Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_euclidean</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Euclidean Distance: 5.385164807134504
</code></pre></div></div> <hr> <p><br></p> <h2 id="chebyshev-distance-max-absolute-difference-in-coordinate-points"><strong>Chebyshev Distance (Max Absolute Difference in Coordinate Points)</strong></h2> <p><img src="https://iq.opengenus.org/content/images/2018/12/chebyshev.png%20" alt="Chebyshev Distance" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"></p> <h4 id="definition-5"><strong>Definition</strong></h4> <ul> <li>For two points $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ and $\mathbf{y} = (y_1, y_2, \ldots, y_n)$ in n-dimensional space, the Chebyshev Distance is defined as:</li> </ul> \[d_{\text{Chebyshev}}(\mathbf{x}, \mathbf{y}) = \max_{1 \le i \le n} \big| x_i - y_i \big|\] <p>Simply put, it is the maximum of the absolute differences between corresponding coordinates.</p> <h4 id="why-and-how-its-used-5"><strong>Why and How It’s Used</strong></h4> <ol> <li> <strong>Maximum Difference</strong>: <ul> <li>Unlike other distances that “sum” or “average” differences across multiple coordinates, Chebyshev distance only focuses on the largest difference.</li> <li>If one coordinate has a particularly large difference while others have small differences, this single largest difference will determine the overall distance.</li> </ul> </li> <li> <strong>Geometric Interpretation</strong>: <ul> <li>On a 2D grid (such as a chessboard), Chebyshev distance can be seen as the minimum number of moves a King needs to move from one square to another, since the King can move one step in any direction (including diagonally).</li> <li>In n-dimensional space, this means being able to move 1 unit simultaneously across multiple coordinates, similar to diagonal movement.</li> </ul> </li> <li> <strong>Application Scenarios</strong>: <ul> <li>Chessboard or grid problems: Calculating the number of moves for a King.</li> <li>Computer games: When diagonal movement is allowed with the same cost as straight movement, Chebyshev distance can measure movement distance.</li> <li>Clustering or anomaly detection: Useful when we want to be highly sensitive to large deviations in any single feature (where a large difference in just one feature makes points distant).</li> </ul> </li> </ol> <h4 id="how-it-works-intuitive-understanding-3"><strong>How It Works (Intuitive Understanding)</strong></h4> <ul> <li> <table> <tbody> <tr> <td>Calculate the absolute difference between corresponding coordinates of the two points: $\big</td> <td>x_i - y_i\big</td> <td>$.</td> </tr> </tbody> </table> </li> <li>Find the maximum value among these differences.</li> <li>This maximum value is the Chebyshev distance.</li> </ul> <h4 id="example-6"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">chebyshev_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>

<span class="c1"># Example usage:
</span><span class="n">point_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">point_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="n">dist_chebyshev</span> <span class="o">=</span> <span class="nf">chebyshev_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Chebyshev Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_chebyshev</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chebyshev Distance: 6
</code></pre></div></div> <h4 id="comparison-between-euclidean-manhattan-chebyshev-distance"><strong>Comparison Between Euclidean/ Manhattan/ Chebyshev Distance</strong></h4> <p><img src="https://iq.opengenus.org/content/images/2018/12/distance.jpg%20" alt="Compare Three Distance" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"></p> <hr> <p><br></p> <p><br></p> <h1 id="scaled-weighted-distance"><strong>Scaled Weighted Distance</strong></h1> <h2 id="manhalanobis-distance-scaled-euclidean-consider-feature-correlation"><strong>Manhalanobis Distance (Scaled Euclidean) (Consider Feature Correlation)</strong></h2> <p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KzsugPQU-BTjvDACXbu9qw.jpeg" alt="Manhalanobis Distance1" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"></p> <h4 id="definition-6"><strong>Definition</strong></h4> <ul> <li>For two vectors (points) $\mathbf{s}$ and $\mathbf{t}$ in d-dimensional space, the Mahalanobis Distance is defined as:</li> </ul> \[d(\mathbf{s}, \mathbf{t}) = \sqrt{(\mathbf{s} - \mathbf{t})^\mathsf{T} \, \mathbf{C}^{-1} \, (\mathbf{s} - \mathbf{t})}\] <p>where $\mathbf{C}$ is the covariance matrix of the data, and $\mathbf{C}^{-1}$ is its inverse matrix.</p> <h4 id="why-and-how-its-used-6"><strong>Why and How It’s Used</strong></h4> <ol> <li> <strong>Accounts for Feature Correlation</strong>: <ul> <li>Unlike ordinary Euclidean distance, Mahalanobis distance considers correlations between different features.</li> <li>If two features are highly correlated, the distance in that direction gets “scaled down.”</li> </ul> </li> <li> <strong>Scale Invariance</strong>: <ul> <li>Automatically adjusts for features with different scales based on covariance. If one feature has a particularly large numeric range, it won’t dominate the distance measure.</li> </ul> </li> <li> <strong>Application Scenarios</strong>: <ul> <li>Outlier detection: Points that differ significantly from the data’s covariance structure are identified as outliers.</li> <li>Classification/Clustering: When dealing with strongly correlated features, using Mahalanobis distance can more accurately reflect true distance relationships.</li> </ul> </li> <li> <strong>Covariates Matching</strong>: <ul> <li>This situation requires a reference subject.</li> <li>There’s a probability that covariates will skew the distance between subjects. <ul> <li>A good solution can be to use the rank of each covariate rather than its value.</li> <li>Or use trimmed mean/median instead of mean.</li> </ul> </li> </ul> </li> </ol> <h4 id="how-it-works-intuitive-understanding-4"><strong>How It Works (Intuitive Understanding)</strong></h4> <ul> <li>First calculate the difference between $\mathbf{s}$ and $\mathbf{t}$.</li> <li>Use the inverse covariance matrix $\mathbf{C}^{-1}$ to scale this difference, measuring its “degree of difference” in terms of data variance and correlation.</li> <li>Finally, take the square root to get the distance.</li> </ul> <h4 id="example-7"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">mahalanobis_distance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">cov</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    s, t: 1D arrays or lists representing points in d-dimensional space
    cov: covariance matrix (d x d)
    </span><span class="sh">"""</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">s</span> <span class="o">-</span> <span class="n">t</span>
    <span class="n">inv_cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>  <span class="c1"># Invert the covariance matrix
</span>    <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">diff</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">inv_cov</span> <span class="o">@</span> <span class="n">diff</span><span class="p">)</span> <span class="c1"># @ is the matrix multiplication operator
</span>    <span class="k">return</span> <span class="n">dist</span>

<span class="c1"># Example usage:
</span><span class="n">point_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="c1"># 2D point can also be np.array([[2, 3],[4,5]]) try yourself
</span><span class="n">point_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>

<span class="c1"># Suppose we know/have an estimated covariance matrix for our 2D data:
</span><span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>  <span class="c1"># Just an example sometimes you don't know
</span>
<span class="n">dist_mahalanobis</span> <span class="o">=</span> <span class="nf">mahalanobis_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">,</span> <span class="n">cov_matrix</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Mahalanobis Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_mahalanobis</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mahalanobis Distance: 2.8784916685156974
</code></pre></div></div> <h4 id="example-2-covariate-matching"><strong>Example 2: Covariate Matching</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">m_distnace</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span><span class="n">xj</span><span class="p">,</span><span class="n">cov</span><span class="p">):</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>
    <span class="n">xj</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">xj</span><span class="p">)</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">xi</span> <span class="o">-</span> <span class="n">xj</span>
    <span class="n">inv_cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">diff</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">inv_cov</span> <span class="o">@</span> <span class="n">diff</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dist</span>

<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">335.285714</span><span class="p">,</span> <span class="mf">4.8095238</span><span class="p">,</span> <span class="mf">3.78571429</span><span class="p">],</span>
                <span class="p">[</span><span class="mf">4.80952381</span><span class="p">,</span> <span class="mf">0.23809524</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0238095</span><span class="p">],</span>
                <span class="p">[</span><span class="mf">3.78571429</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0238095</span><span class="p">,</span> <span class="mf">0.28571429</span><span class="p">]])</span>

<span class="c1"># The Treatment column is not a feature for distance computation because it represents a categorical assignment rather than a characteristic.
</span>
<span class="c1"># Data from the table (Age, College, Male) # covariance is assumed not fixed params
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">68</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>  <span class="c1"># Subject 1 (reference)
</span>    <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>  <span class="c1"># Subject 2
</span>    <span class="p">[</span><span class="mi">65</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>  <span class="c1"># Subject 3
</span>    <span class="p">[</span><span class="mi">76</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>  <span class="c1"># Subject 4
</span>    <span class="p">[</span><span class="mi">44</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>  <span class="c1"># Subject 5
</span>    <span class="p">[</span><span class="mi">34</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>  <span class="c1"># Subject 6
</span>    <span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>   <span class="c1"># Subject 7
</span><span class="p">])</span>

<span class="c1"># Define xi (Reference Subject, Subject 1)
</span><span class="n">xi</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Reference subject (Subject 1)
</span>
<span class="c1"># Define xj (Other Subjects)
</span><span class="n">xj</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>  <span class="c1"># Other subjects (Subjects 2 to 7)
</span>
<span class="c1"># Compute distances from Subject 1 to all other subjects
</span><span class="n">distances</span> <span class="o">=</span> <span class="p">[</span><span class="nf">mahalanobis_distance</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">xj</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span> <span class="k">for</span> <span class="n">xj</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>

<span class="c1"># Print results
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mahalanobis Distance d_M(1, </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">) = </span><span class="si">{</span><span class="n">d</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mahalanobis Distance d_M(1, 2) = 2.88
Mahalanobis Distance d_M(1, 3) = 0.23
Mahalanobis Distance d_M(1, 4) = 2.31
Mahalanobis Distance d_M(1, 5) = 2.00
Mahalanobis Distance d_M(1, 6) = 2.36
Mahalanobis Distance d_M(1, 7) = 3.03
</code></pre></div></div> <hr> <p><br></p> <h2 id="canberra-distance-weighted-manhattan"><strong>Canberra Distance (Weighted Manhattan)</strong></h2> <p>(Use for Text analysis or Gene Expression Data)</p> <p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs44147-024-00535-2/MediaObjects/44147_2024_535_Fig11_HTML.png?as=webp" alt="Canberra Distance" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"></p> <h4 id="definition-7"><strong>Definition</strong></h4> <ul> <li> <p>For two d-dimensional vectors $\mathbf{s} = (s_1, s_2, …, s_d)$ and $\mathbf{t} = (t_1, t_2, …, t_d)$, the Canberra distance is defined as:</p> </li> <li> <p>Standard equation:</p> </li> </ul> \[d(\mathbf{s}, \mathbf{t}) = \sum_{j=1}^{d} \frac{|s_j - t_j|}{|s_j| + |t_j|}\] <ul> <li>Sometimes with variation, e.g. normalization:</li> </ul> \[d(\mathbf{s}, \mathbf{t}) = \frac{1}{d} \sum_{j=1}^{d} \frac{|s_j - t_j|}{|s_j| + |t_j|}\] <p>Sometimes a factor of 2 is seen in the formula (either in front or inside), but the core idea is that it’s a weighted version of Manhattan distance.</p> <h4 id="why-and-how-its-used-7"><strong>Why and How It’s Used</strong></h4> <ol> <li> <strong>Weighted by Magnitude</strong>: <ul> <li> <table> <tbody> <tr> <td>Each coordinate difference $</td> <td>s_j - t_j</td> <td>$ is divided by $</td> <td>s_j</td> <td>+</td> <td>t_j</td> <td>$.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>If both $</td> <td>s_j</td> <td>$ and $</td> <td>t_j</td> <td>$ are large, the difference is scaled down; if one value is small or zero, the difference is amplified.</td> </tr> </tbody> </table> </li> </ul> </li> <li> <strong>Very Sensitive to Small Values</strong>: <ul> <li> <table> <tbody> <tr> <td>If a coordinate is near 0, then $</td> <td>s_j</td> <td>+</td> <td>t_j</td> <td>$ is very small, which causes that term’s distance value to become large.</td> </tr> </tbody> </table> </li> <li>This makes Canberra distance very sensitive to changes in features with small or zero values.</li> </ul> </li> <li> <strong>Application Scenarios</strong>: <ul> <li>Data such as text analysis or gene expression, where zero or near-zero counts are very important.</li> <li>Canberra distance can be used when we are more concerned with relative differences rather than absolute differences.</li> </ul> </li> </ol> <h4 id="how-it-works-intuitive-understanding-5"><strong>How It Works (Intuitive Understanding)</strong></h4> <ul> <li> <table> <tbody> <tr> <td>For each coordinate j, first compute the absolute difference $</td> <td>s_j - t_j</td> <td>$.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Then divide by the sum $</td> <td>s_j</td> <td>+</td> <td>t_j</td> <td>$. If this sum is small, then that coordinate’s contribution to the overall distance becomes larger.</td> </tr> </tbody> </table> </li> <li>Sum the results for all dimensions to get the Canberra distance.</li> </ul> <p>Compute $\frac{|1 - 2|}{|1| + |2|} + \frac{|10 - 5|}{|10| + |5|} + \frac{|0 - 3|}{|0| + |3|}$. Each dimension, due to the different magnitudes of the denominators, contributes differently to the overall distance.</p> <h4 id="example-8"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">canberra_distance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="c1"># To avoid division by zero, we can handle 0 in the denominator carefully:
</span>    <span class="c1"># We'll replace 0 with a small epsilon or skip that term if both s_j and t_j are 0.
</span>    <span class="c1"># Here, let's just do a safe division approach:
</span>    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-12</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="p">(</span><span class="n">denominator</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">ratio</span><span class="p">)</span>

<span class="c1"># Example usage:
</span><span class="n">point_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">point_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>

<span class="n">dist_canberra</span> <span class="o">=</span> <span class="nf">canberra_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Canberra Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_canberra</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Canberra Distance: 1.6666666666661998
</code></pre></div></div> <p><br></p> <hr> <h2 id="final-note"><strong>Final Note:</strong></h2> <p>I believe it is very important to restate that all distances included in this blog post are the common ones that I used at least onece in either one of my school or work projects. So be aware that are many new distance measures that you may encounter in doing your own projects, don’t hesitate to learn.</p> <h4 id="many-distances-comparison"><strong>Many Distances Comparison</strong></h4> <p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UBVod31pjOcv41LJrBC7lg.jpeg" alt="Many Distances Comparison" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"></p> <h2 id="image-reference"><strong>Image Reference</strong></h2> <ol> <li><a href="https://statistics.laerd.com/statistical-guides/img/spearman-1-small.png" rel="external nofollow noopener" target="_blank">Spearman Correlation</a></li> <li><a href="https://media.geeksforgeeks.org/wp-content/uploads/20250723175534566635/pearson_correlation_coefficient.webp" rel="external nofollow noopener" target="_blank">Pearson Correlation</a></li> <li><a href="https://media.geeksforgeeks.org/wp-content/uploads/20200911171455/UntitledDiagram2.png" rel="external nofollow noopener" target="_blank">Cosine Similarity</a></li> <li><a href="https://www.kdnuggets.com/wp-content/uploads/c_distance_metrics_euclidean_manhattan_minkowski_oh_12.jpg" rel="external nofollow noopener" target="_blank">Minkowski Distance</a></li> <li><a href="https://rosalind.info/media/Euclidean_distance.png" rel="external nofollow noopener" target="_blank">Euclidean Distance</a></li> <li><a href="https://iq.opengenus.org/content/images/2018/12/distance.jpg" rel="external nofollow noopener" target="_blank">Compare Three Distances</a></li> <li><a href="https://iq.opengenus.org/content/images/2018/12/chebyshev.png" rel="external nofollow noopener" target="_blank">Chebyshev Distance</a></li> <li><a href="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KzsugPQU-BTjvDACXbu9qw.jpeg" rel="external nofollow noopener" target="_blank">Mahalanobis Distance1</a></li> <li><a href="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UBVod31pjOcv41LJrBC7lg.jpeg" rel="external nofollow noopener" target="_blank">Many Distances Comparison</a></li> <li><a href="https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs44147-024-00535-2/MediaObjects/44147_2024_535_Fig11_HTML.png?as=webp" rel="external nofollow noopener" target="_blank">Canberra Distance</a></li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/bag-of-words-2/">Practical Applications of the Bag of Words Model</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/bag-of-words-1/">The Bag of Words Model, A Comprehensive Analysis of NLP's Foundational Technique</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Qirui(Micheli) Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>