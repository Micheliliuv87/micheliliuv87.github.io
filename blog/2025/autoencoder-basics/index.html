<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Autoencoder Basics and How to Implement | Qirui(Micheli) Liu </title> <meta name="author" content="Qirui(Micheli) Liu"> <meta name="description" content="Simple implementation of autoencoder framework"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/MickVicon.jpeg?4228235fca6310e5e9d7247ffa5842ef"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://micheliliuv87.github.io/blog/2025/autoencoder-basics/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Autoencoder Basics and How to Implement",
            "description": "Simple implementation of autoencoder framework",
            "published": "May 14, 2025",
            "authors": [
              
              {
                "author": "Micheli Liu",
                "authorURL": "https://micheliliuv87.github.io/",
                "affiliations": [
                  {
                    "name": "Emory University ISOM",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Qirui(Micheli)</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Autoencoder Basics and How to Implement</h1> <p>Simple implementation of autoencoder framework</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#sparse-autoencoder-sae">Sparse Autoencoder (SAE)</a> </div> </nav> </d-contents> <h1 id="still-updating">(Still Updating)</h1> <h2 id="sparse-autoencoder-sae"><strong>Sparse Autoencoder (SAE)</strong></h2> <h3 id="definition"><strong>Definition:</strong></h3> <p>A sparse autoencoder is a type of neural network used to learn compact, interpretable representations of data by enforcing sparsity in its hidden layer activations. Unlike regular autoencoders that mainly compress and reconstruct data, sparse autoencoders add a sparsity penalty (such as L1 regularization or KL divergence) to encourage only a small subset of neurons to be activated at any given time, leading to extraction of the most salient features and helping prevent overfitting. (Check out very interesting <a href="https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf" rel="external nofollow noopener" target="_blank">lecture notes</a> by Andrew Ng)</p> <h3 id="understanding-in-simple-words--implementation"><strong>Understanding in Simple Words &amp; Implementation</strong></h3> <ul> <li> <p>“A SAE enforces sparsity in the hidden layer activations”</p> <ul> <li>Regular autoencoders minimize reconstruction error only.</li> <li>Sparse autoencoders add a constraint: they penalize the model when too many hidden units are active.</li> <li>This pushes the model to keep most neuron activations near zero.</li> </ul> </li> <li> <p>“The idea is to make most of the neurons inactive, forcing the model to learn efficient feature representations.”</p> <ul> <li>Focus on a few features at a time,</li> <li>Learn more interpretable and robust representations,</li> <li>Avoid overfitting or trivial identity mappings.</li> </ul> </li> <li> <p><strong>Loss Function</strong>: \(\text{L} = \sum_{i=1}^{n} \|X_i - X_i{\prime}\|^2 + \lambda \sum |z_i|\)</p> </li> </ul> <p><strong>This loss function does two things:</strong></p> <ol> <li> <p>Minimizes reconstruction error so that the autoencoder can accurately represent the input.</p> </li> <li> <p>Adds an L1 penalty on the hidden activations z_i, encouraging sparse activations.</p> </li> </ol> <p>This is similar to how Lasso regression enforces sparsity in coefficients.</p> <p><br></p> <table> <thead> <tr> <th>Symbol</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>$X_i$</td> <td>The original input sample</td> </tr> <tr> <td>$X_i’$</td> <td>The reconstructed output (decoded from latent representation)</td> </tr> <tr> <td>$|X_i - X_i’|^2$</td> <td>Reconstruction loss: how close the output is to the input</td> </tr> <tr> <td>$z_i$</td> <td>Hidden layer activation for input ( i )</td> </tr> <tr> <td>$\sum \lvert z_i \rvert$</td> <td>Sum of absolute hidden activations — the sparsity penalty</td> </tr> <tr> <td>$\lambda$</td> <td>Regularization parameter controlling the strength of sparsity</td> </tr> </tbody> </table> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1"># 1) Define the Sparse Autoencoder
</span><span class="k">class</span> <span class="nc">SparseAutoencoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># encoder: input → hidden
</span>        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="c1"># decoder: hidden → reconstruction
</span>        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="c1"># encode with sigmoid activation
</span>        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="c1"># decode with sigmoid (or no activation, depending on your data)
</span>        <span class="n">x_recon</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x_recon</span><span class="p">,</span> <span class="n">z</span>

<span class="c1"># 2) Hyperparameters
</span><span class="n">input_dim</span>    <span class="o">=</span> <span class="mi">784</span>   <span class="c1"># e.g. flattened 28×28 image
</span><span class="n">hidden_dim</span>   <span class="o">=</span> <span class="mi">64</span>    <span class="c1"># size of latent (bottleneck)
</span><span class="n">lambda_sparse</span> <span class="o">=</span> <span class="mf">1e-3</span> <span class="c1"># weight of the sparsity penalty
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>

<span class="c1"># 3) Instantiate model, loss &amp; optimizer
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">SparseAutoencoder</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
<span class="n">mse_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># 4) Dummy data loader (replace with real dataset)
#    Here: 100 samples of dimension 784
</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>

<span class="c1"># 5) Training loop (one epoch for illustration)
</span><span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>            <span class="c1"># if you have a real DataLoader, iterate batches
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>    <span class="c1"># make it shape (1, 784)
</span>    
    <span class="c1"># Forward pass
</span>    <span class="n">x_recon</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># Reconstruction loss
</span>    <span class="n">recon_loss</span> <span class="o">=</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="n">x_recon</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># Sparsity loss: L1 on hidden activations
</span>    <span class="c1"># Sum over hidden dims, mean over batch
</span>    <span class="n">sparsity_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    
    <span class="c1"># Total loss
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">recon_loss</span> <span class="o">+</span> <span class="n">lambda_sparse</span> <span class="o">*</span> <span class="n">sparsity_loss</span>
    
    <span class="c1"># Backpropagation
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Recon: </span><span class="si">{</span><span class="n">recon_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">  |  Sparse: </span><span class="si">{</span><span class="n">sparsity_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">  |  Total: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Recon: 1.2073  |  Sparse: 0.5127  |  Total: 1.2078
Recon: 1.3084  |  Sparse: 0.5057  |  Total: 1.3089
Recon: 1.1943  |  Sparse: 0.5150  |  Total: 1.1949
Recon: 1.3257  |  Sparse: 0.4914  |  Total: 1.3262
Recon: 1.1187  |  Sparse: 0.4987  |  Total: 1.1192
Recon: 1.0752  |  Sparse: 0.4697  |  Total: 1.0756
Recon: 1.1239  |  Sparse: 0.5137  |  Total: 1.1244
...
</code></pre></div></div> <hr> <h2 id="references"><strong>References</strong></h2> <ol> <li><a href="https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf" rel="external nofollow noopener" target="_blank">Sparse Autoencoder Lecture Notes by Andrew Ng</a></li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://www.voiceofgoizueta.com/winning-with-data-goizueta-msba-students-shine-in-travelers-university-modeling-competition/" target="_blank" rel="external nofollow noopener">Winning with Data: Goizueta MSBA Students Shine in Travelers University Modeling Competition - Voice of Goizueta</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/oop-class/">OOP Class Tutorial 1</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/distance-measure/">Distance Measures for Data Science</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/bag-of-words-2/">Practical Applications of the Bag of Words Model</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/bag-of-words-1/">The Bag of Words Model, A Comprehensive Analysis of NLP's Foundational Technique</a> </li> <br> <br> <div id="giscus_thread"> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Qirui(Micheli) Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>