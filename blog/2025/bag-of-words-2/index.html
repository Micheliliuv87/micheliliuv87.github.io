<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Practical Applications of the Bag of Words Model | Qirui(Micheli) Liu </title> <meta name="author" content="Qirui(Micheli) Liu"> <meta name="description" content="Application of Bag of words, examples of TF-IDF, Text Classification, and Sentiment Analysis"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/MickVicon.jpeg?4228235fca6310e5e9d7247ffa5842ef"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://micheliliuv87.github.io/blog/2025/bag-of-words-2/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Qirui(Micheli)</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Practical Applications of the Bag of Words Model</h1> <p class="post-meta"> Created on January 05, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/tutorials"> <i class="fa-solid fa-hashtag fa-sm"></i> Tutorials</a>   ·   <a href="/blog/category/text"> <i class="fa-solid fa-tag fa-sm"></i> Text</a>   <a href="/blog/category/processing"> <i class="fa-solid fa-tag fa-sm"></i> Processing</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="introduction--recap"><strong>Introduction &amp; Recap</strong></h2> <p>In our previous post, we explored the <strong>theoretical foundations</strong> of the Bag of Words (BoW) model—how it converts unstructured text into numerical representations by counting word frequencies while disregarding word order and grammar. This fundamental technique has proven remarkably effective across various <strong>Natural Language Processing (NLP) tasks</strong> despite its simplicity .</p> <p>Now, let’s transition from theory to practice. In this hands-on guide, we’ll implement the Bag of Words model for several real-world applications including text classification, sentiment analysis, and TF-IDF visualization. We’ll work with Python libraries like <code class="language-plaintext highlighter-rouge">scikit-learn</code>, <code class="language-plaintext highlighter-rouge">NLTK</code>, and <code class="language-plaintext highlighter-rouge">matplotlib</code> to build working examples you can adapt for your own projects.</p> <h2 id="text-classification-with-bow-spam-detection-example"><strong>Text Classification with BoW: Spam Detection Example</strong></h2> <p>One of the most common applications of Bag of Words is <strong>text classification</strong>, where we categorize documents into predefined classes. Let’s build a spam detection system that classifies messages as “spam” or “not spam.”</p> <h4 id="step-by-step-implementation"><strong>Step-by-Step Implementation</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">re</span>
<span class="kn">import</span> <span class="n">nltk</span>
<span class="kn">from</span> <span class="n">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="n">nltk.stem</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>
<span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># download NLTK sample data
</span><span class="n">nltk</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="sh">'</span><span class="s">stopwords</span><span class="sh">'</span><span class="p">)</span>
<span class="n">nltk</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="sh">'</span><span class="s">punkt</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># sample dataset (in practice, you'd use a larger dataset)
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">Win a free iPhone now! Click here to claim your prize.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Your package has been delivered. Track your shipment.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Congratulations! You won a $1000 gift card. Reply to claim.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Meeting scheduled for tomorrow at 10 AM in conference room.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Urgent! Your account needs verification. Update immediately.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">The quarterly report is attached for your review.</span><span class="sh">'</span>
    <span class="p">],</span>
    <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">not spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">not spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">not spam</span><span class="sh">'</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># text preprocessing function
</span><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># remove non-alphabetic characters and convert to lowercase
</span>    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">[^A-Za-z]</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>

    <span class="c1">#tokenize
</span>    <span class="n">words</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="c1">#remove stopwords
</span>    <span class="n">stop_words</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">))</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>

    <span class="c1">#apply stemming
</span>    <span class="n">stemmer</span> <span class="o">=</span> <span class="nc">PorterStemmer</span><span class="p">()</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="p">.</span><span class="nf">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>

    <span class="k">return</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="c1"># apply preprocessing
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">processed_text</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span><span class="n">preprocess_text</span><span class="p">)</span>

<span class="c1"># create Bag of Words representation
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">processed_text</span><span class="sh">'</span><span class="p">]).</span><span class="nf">toarray</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># split data into training and testing sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># train a Naive Bayes classifier
</span><span class="n">classifier</span> <span class="o">=</span> <span class="nc">GaussianNB</span><span class="p">()</span>
<span class="n">classifier</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># make predictions and evaluate accuracy
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Model Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h4 id="how-bow-helps-in-classification"><strong>How BoW Helps in Classification</strong></h4> <p>In this spam detection example, the Bag of Words model identifies <strong>characteristic word patterns</strong> in spam versus legitimate messages . Spam messages often contain words like “win,” “free,” “prize,” and “urgent,” while legitimate messages use more neutral language. By converting these word patterns into numerical features, we enable machine learning algorithms to learn the distinguishing characteristics of each category.</p> <p>The <code class="language-plaintext highlighter-rouge">CountVectorizer</code> from scikit-learn handles the heavy lifting of creating our BoW representation . The <code class="language-plaintext highlighter-rouge">max_features</code> parameter ensures we only consider the top 1000 most frequent words, preventing excessively high-dimensional data.</p> <h2 id="sentiment-analysis-with-bow"><strong>Sentiment Analysis with BoW</strong></h2> <p>Another powerful application of Bag of Words is <strong>sentiment analysis</strong>—determining whether a piece of text expresses positive, negative, or neutral sentiment. Let’s analyze movie reviews.</p> <h4 id="implementation-code"><strong>Implementation Code</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Sample movie reviews dataset
</span><span class="n">reviews</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">This movie was absolutely fantastic! Great acting and plot.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Terrible waste of time. Poor acting and boring story.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Loved the cinematography and character development.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">The worst movie I have ever seen in my life.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Brilliant performance by the lead actor. Highly recommended.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Mediocre at best. Nothing special about this film.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">An outstanding masterpiece that kept me engaged throughout.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Poor direction and weak screenplay. Very disappointing.</span><span class="sh">'</span>
<span class="p">]</span>

<span class="n">sentiments</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">positive</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">positive</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">positive</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">positive</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># Create BoW model with unigrams and bigrams
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">reviews</span><span class="p">)</span>

<span class="c1"># Train a classifier
</span><span class="n">classifier</span> <span class="o">=</span> <span class="nc">MultinomialNB</span><span class="p">()</span>
<span class="n">classifier</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">sentiments</span><span class="p">)</span>

<span class="c1"># Test with new reviews
</span><span class="n">test_reviews</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">The acting was good but the story was weak</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Amazing movie with fantastic performances</span><span class="sh">'</span>
<span class="p">]</span>

<span class="n">test_vectors</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">test_reviews</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">test_vectors</span><span class="p">)</span>

<span class="k">for</span> <span class="n">review</span><span class="p">,</span> <span class="n">sentiment</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">test_reviews</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Review: </span><span class="sh">'</span><span class="si">{</span><span class="n">review</span><span class="si">}</span><span class="sh">'</span><span class="s"> -&gt; Sentiment: </span><span class="si">{</span><span class="n">sentiment</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h4 id="understanding-the-output"><strong>Understanding the Output</strong></h4> <p>This example introduces an important enhancement: <strong>ngrams</strong> . By setting <code class="language-plaintext highlighter-rouge">ngram_range=(1, 2)</code>, we consider both single words (unigrams) and pairs of consecutive words (bigrams). This helps capture phrases like “absolutely fantastic” or “poor acting” that carry more nuanced sentiment than individual words.</p> <p>The <code class="language-plaintext highlighter-rouge">MultinomialNB</code> classifier is particularly well-suited for text classification with discrete features like word counts . It efficiently learns the probability distributions of words for each sentiment class.</p> <h2 id="tf-idf-enhancing-bag-of-words"><strong>TF-IDF: Enhancing Bag of Words</strong></h2> <p>While simple word counts are useful, they have a limitation: common words that appear frequently across all documents may dominate the analysis. <strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong> addresses this by weighting words based on their importance .</p> <h4 id="tf-idf-calculation-and-visualization"><strong>TF-IDF Calculation and Visualization</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># sample documents for TF-IDF demonstration
</span><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">Natural language processing with Python is fascinating and powerful</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Python is a great programming language for data science</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">I enjoy learning new things about NLP and Python programming</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Machine learning and artificial intelligence are changing the world</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Deep learning models require extensive computational resources</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Data scientists use Python for machine learning projects</span><span class="sh">"</span>
<span class="p">]</span>

<span class="c1"># calculate TF-IDF
</span><span class="n">tfidf_vectorizer</span> <span class="o">=</span> <span class="nc">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">)</span>
<span class="n">tfidf_matrix</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

<span class="c1"># get feature names and TF-IDF scores
</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">()</span>
<span class="n">dense_matrix</span> <span class="o">=</span> <span class="n">tfidf_matrix</span><span class="p">.</span><span class="nf">todense</span><span class="p">()</span>

<span class="c1"># display TF-IDF scores for first document
</span><span class="n">first_doc</span> <span class="o">=</span> <span class="n">dense_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">word_scores</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">first_doc</span><span class="p">),</span>
                    <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Top terms in first document:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">word_scores</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1">#dimension reduction for visualization
</span><span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca_result</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">dense_matrix</span><span class="p">)</span>

<span class="c1"># create visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">pca_result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">pca_result</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="c1">#ddd document labels
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([</span><span class="sa">f</span><span class="sh">"</span><span class="s">Doc </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">documents</span><span class="p">))]):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">pca_result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">pca_result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                 <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="sh">'</span><span class="s">offset points</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">TF-IDF Document Visualization using PCA</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Principal Component 1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Principal Component 2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h4 id="tf-idf-intuition"><strong>TF-IDF Intuition</strong></h4> <p>TF-IDF balances two factors :</p> <ul> <li> <strong>Term Frequency (TF)</strong>: How often a word appears in a specific document</li> <li> <strong>Inverse Document Frequency (IDF)</strong>: How rare the word is across all documents</li> </ul> <p>The product of these two values gives higher weight to words that are frequent in a specific document but rare in the overall collection. This effectively identifies words that are characteristic of each document.</p> <p><img src="https://i.ytimg.com/vi/zLMEnNbdh4Q/maxresdefault.jpg" alt="TF-IDF Visualization" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"></p> <p><em>TF-IDF helps identify the most distinctive words in documents, pushing similar documents closer together in vector space .</em></p> <h2 id="advanced-techniques--best-practices"><strong>Advanced Techniques &amp; Best Practices</strong></h2> <h4 id="feature-engineering-with-n-grams"><strong>Feature Engineering with N-grams</strong></h4> <p>As mentioned in the sentiment analysis example, n-grams can significantly improve model performance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># comparing different n-gram ranges
</span><span class="n">unigram_vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">bigram_vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">trigram_vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># each approach captures different linguistic patterns
</span></code></pre></div></div> <p>Bigrams and trigrams help preserve contextual information that single words might lose . For example, the phrase “not good” has a very different meaning than the individual words “not” and “good” considered separately.</p> <h4 id="handling-large-vocabularies-with-feature-hashing"><strong>Handling Large Vocabularies with Feature Hashing</strong></h4> <p>For extremely large datasets, the BoW representation can become computationally challenging. <strong>Feature hashing</strong> (or the “hash trick”) provides a memory-efficient alternative :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">HashingVectorizer</span>

<span class="c1"># using hashing vectorizer for large datasets
</span><span class="n">hash_vectorizer</span> <span class="o">=</span> <span class="nc">HashingVectorizer</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">alternate_sign</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">X_hash</span> <span class="o">=</span> <span class="n">hash_vectorizer</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre></div></div> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>The Bag of Words model, while simple, remains remarkably powerful for practical text analysis tasks. Through our implementations, we’ve seen how BoW enables:</p> <ul> <li> <strong>Text classification</strong> by identifying characteristic word patterns in different categories</li> <li> <strong>Sentiment analysis</strong> by capturing emotional language through word frequencies</li> <li> <strong>Enhanced analysis with TF-IDF</strong> by weighting words based on their distinctiveness</li> </ul> <p>The true power of Bag of Words lies in its <strong>versatility and interpretability</strong>. Unlike more complex deep learning models, BoW features are easily understandable and can provide valuable insights into what the model is learning.</p> <p>While modern approaches like word embeddings and transformer models have their place for complex NLP tasks, Bag of Words remains an excellent starting point for most text analysis projects—offering a compelling balance of simplicity, interpretability, and effectiveness.</p> <p><em>All code examples in this post are designed to be runnable with Python 3.6+ and standard data science libraries (pandas, scikit-learn, NLTK, matplotlib).</em></p> <hr> <p><em>This practical guide builds upon the theoretical foundations established in our previous post about the Bag of Words model. Implement these techniques as a starting point for your text analysis projects, then experiment with different preprocessing approaches and parameter tuning to optimize for your specific use cases.</em></p> <h2 id="references"> <strong>References</strong>:</h2> <ol> <li><a href="https://www.datacamp.com/tutorial/python-bag-of-words-model" rel="external nofollow noopener" target="_blank">Python Bag of Words Models</a></li> <li><a href="https://www.geeksforgeeks.org/machine-learning/visualizing-tf-idf-scores-a-comprehensive-guide-to-plotting-a-document-tf-idf-2d-graph/" rel="external nofollow noopener" target="_blank">Visualizing TF-IDF Scores: A Comprehensive Guide to Plotting a Document TF-IDF 2D Graph</a></li> <li><a href="https://medium.com/swlh/text-classification-using-the-bag-of-words-approach-with-nltk-and-scikit-learn-9a731e5c4e2f" rel="external nofollow noopener" target="_blank">Text classification using the Bag Of Words Approach with NLTK and Scikit Learn</a></li> <li><a href="https://medium.com/@GeoffreyGordonAshbrook/vector-visualization-2d-plot-your-tf-idf-with-pca-83fa9fccb1d" rel="external nofollow noopener" target="_blank">Vector Visualization: 2D Plot your TF-IDF with PCA</a></li> <li><a href="https://www.geeksforgeeks.org/nlp/bag-of-words-bow-model-in-nlp/" rel="external nofollow noopener" target="_blank">Bag of words (BoW) model in NLP</a></li> <li><a href="https://www.analyticsvidhya.com/blog/2021/08/a-friendly-guide-to-nlp-bag-of-words-with-python-example/" rel="external nofollow noopener" target="_blank">A friendly guide to NLP: Bag-of-Words with Python example</a></li> <li><a href="https://www.tidytextmining.com/tfidf" rel="external nofollow noopener" target="_blank">3 Analyzing word and document frequency: tf-idf</a></li> <li><a href="https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf" rel="external nofollow noopener" target="_blank">Analyzing Documents with TF-IDF</a></li> <li><a href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/" rel="external nofollow noopener" target="_blank">A Gentle Introduction to the Bag-of-Words Model</a></li> <li><a href="https://blog.quantinsti.com/bag-of-words/" rel="external nofollow noopener" target="_blank">Bag of Words: Approach, Python Code, Limitations</a></li> </ol> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Liu, Qirui(Micheli) (Jan 2025). Practical Applications of the Bag of Words Model. https://micheliliuv87.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025practical-applications-of-the-bag-of-words-model</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Practical Applications of the Bag of Words Model}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Liu, Qirui(Micheli)}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Jan}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://micheliliuv87.github.io/blog/2025/bag-of-words-2/}</span>
<span class="p">}</span>
</code></pre></div></div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/distance-measure/">Distance Measures for Data Science</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/distance-measure-copy/">Autoencoder Basics and How to Implement</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/bag-of-words-1/">The Bag of Words Model, A Comprehensive Analysis of NLP's Foundational Technique</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Qirui(Micheli) Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>