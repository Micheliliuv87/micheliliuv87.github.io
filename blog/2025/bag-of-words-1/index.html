<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Bag of Words Model, A Comprehensive Analysis of NLP's Foundational Technique | Qirui(Micheli) Liu </title> <meta name="author" content="Qirui(Micheli) Liu"> <meta name="description" content="BoW introduction, history, and use cases"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/MickVicon.jpeg?4228235fca6310e5e9d7247ffa5842ef"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://micheliliuv87.github.io/blog/2025/bag-of-words-1/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Bag of Words Model, A Comprehensive Analysis of NLP's Foundational Technique",
            "description": "BoW introduction, history, and use cases",
            "published": "January 02, 2025",
            "authors": [
              
              {
                "author": "Qirui(Micheli) Liu",
                "authorURL": "https://micheliliuv87.github.io/",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Qirui(Micheli)</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Bag of Words Model, A Comprehensive Analysis of NLP's Foundational Technique</h1> <p>BoW introduction, history, and use cases</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#"></a> </div> </nav> </d-contents> <h2 id="introduction-to-bow"><strong>Introduction to BoW</strong></h2> <p>In the landscape of <strong>Natural Language Processing</strong> (NLP), few models have been as fundamentally important and enduringly influential as the <strong>Bag of Words</strong> (<a href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="external nofollow noopener" target="_blank">BoW</a>) model. At its core, BoW represents a straightforward yet powerful approach to text representation: it transforms unstructured text into a structured, numerical format by treating a document as an unordered collection of words while tracking their frequency. This conceptual simplicity has made BoW a <strong>foundational technique</strong> that continues to serve as a baseline for text classification and feature extraction tasks, despite the emergence of more sophisticated alternatives.</p> <p>The BoW model operates on a fundamental assumption that the frequency of words in a document captures meaningful information about its content, regardless of their order or grammatical relationships . This approach might seem counterintuitive—after all, human language relies heavily on word order and syntax for meaning—yet BoW has proven remarkably effective for many <a href="https://builtin.com/machine-learning/bag-of-words" rel="external nofollow noopener" target="_blank">practical NLP applications</a>, from spam detection to sentiment analysis.</p> <h2 id="history-in-short"><strong>History in Short</strong></h2> <p>The conceptual origins of Bag of Words trace back to the mid-20th century, with early references found in Zellig Harris’s 1954 article on “Distributional Structure” . The model emerged from the intersection of <strong>computational linguistics</strong> and <strong>information retrieval</strong> during the 1950s, when researchers sought pragmatic solutions for processing text data without needing to understand complex grammatical structures.</p> <p>Initially developed in the context of document classification and early information retrieval systems, BoW gained significant traction as researchers recognized that <strong>word frequency patterns</strong> could provide substantial insights into document content and categorization . By the 1990s, BoW had become a standard technique in natural language processing, finding robust applications in spam filtering, document classification, and early sentiment analysis systems.</p> <p>The mathematical foundation of BoW—representing documents as vectors where each dimension corresponds to a unique word and the value represents its frequency—revolutionized text analysis by enabling computational systems to perform mathematical operations on textual data . This transformation from unstructured text to structured numerical representation opened new possibilities for machine learning applications in natural language.</p> <h2 id="how-bag-of-words-works"><strong>How Bag of Words Works?</strong></h2> <h4 id="central-mechanism"><strong>Central Mechanism</strong></h4> <p>The Bag of Words model transforms text through a systematic process that disregards word order and syntax while preserving information about word occurrence and frequency . The standard implementation involves three key steps:</p> <ol> <li> <strong>Tokenization</strong>: Breaking down text into individual words or tokens</li> <li> <strong>Vocabulary Creation</strong>: Building a unique dictionary of all distinct words across the corpus</li> <li> <strong>Vectorization</strong>: Converting each document into a numerical vector based on word frequencies</li> </ol> <h4 id="practical-implementation"><strong>Practical Implementation</strong></h4> <p>Here’s a concrete example that illustrates the BoW process:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="c1"># sample documents
</span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">I love programming and programming is fun</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">I love machine learning</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Machine learning is fascinating</span><span class="sh">'</span>
<span class="p">]</span>

<span class="c1"># build and fit vectorizer
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="c1"># print
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Vocabulary:</span><span class="sh">"</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">BoW matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="nf">toarray</span><span class="p">())</span>
</code></pre></div></div> <p>You would expect the following output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Vocabulary: ['and' 'fascinating' 'fun' 'is' 'learning' 'love' 'machine' 'programming']
BoW matrix:
 [[1 0 1 1 0 1 0 2]
 [0 0 0 0 1 1 1 0]
 [0 1 0 1 1 0 1 0]]
</code></pre></div></div> <p>The resulting matrix represents each document as a vector where each element corresponds to the frequency of a specific word from the vocabulary . The first document, for instance, contains the word “and” once, “fun” once, “is” once, “love” once, and “programming” twice.</p> <p>The BoW model creates what can be described as a <strong>vector space</strong> where each unique word becomes a separate dimension . Documents are then plotted as points in this multi-dimensional space, with their positions along each dimension determined by the frequency of the corresponding word. This representation enables mathematical comparison and analysis of documents based on their word distribution patterns.</p> <h2 id="applications-and-use-cases"><strong>Applications and Use Cases</strong></h2> <p>The Bag of Words model has found diverse applications across numerous domains of text analysis and machine learning:</p> <h4 id="text-classification"><strong>Text Classification</strong></h4> <p>BoW serves as a fundamental feature extraction technique for <strong>document categorization</strong> tasks. Email services extensively use BoW for spam detection by analyzing the frequency of specific words indicative of spam content . Similarly, news organizations employ BoW for <strong>topic classification</strong>, automatically categorizing articles based on their predominant vocabulary .</p> <h4 id="sentiment-analysis"><strong>Sentiment Analysis</strong></h4> <p>Companies leverage BoW to understand <strong>customer sentiment</strong> across reviews, social media, and feedback platforms. By mapping word frequencies to positive or negative sentiment indicators, businesses can gauge public opinion about products or services at scale . For instance, words like “awful” or “terrible” appearing frequently in product reviews strongly indicate negative sentiment, while “excellent” or “amazing” suggest positive experiences .</p> <h4 id="information-retrieval"><strong>Information Retrieval</strong></h4> <p>Early search engines relied heavily on BoW principles to match user queries with relevant documents . While modern search algorithms have incorporated more sophisticated techniques, the fundamental approach of measuring <strong>word frequency</strong> and <strong>presence</strong> remains crucial in information retrieval systems.</p> <h4 id="other-applications"><strong>Other Applications</strong></h4> <ul> <li> <strong>Document similarity detection</strong>: Identifying similar documents based on shared word distribution patterns</li> <li> <strong>Language identification</strong>: Determining the language of a document based on characteristic vocabulary</li> <li> <strong>Recommendation systems</strong>: Analyzing product descriptions or user reviews to generate personalized recommendations</li> <li> <strong>Text clustering</strong>: Grouping similar documents together without predefined categories</li> </ul> <h2 id="limitations-and-challenges"><strong>Limitations and Challenges</strong></h2> <p>Despite its widespread adoption and utility, the Bag of Words model faces several significant limitations:</p> <h4 id="contextual-understanding"><strong>Contextual Understanding</strong></h4> <p>The most notable drawback of BoW is its <strong>complete disregard for word order</strong> and contextual relationships . This limitation means that sentences with identical words but different meanings receive identical representations. For example, “Man bites dog” and “Dog bites man” are treated as the same by BoW, despite their dramatically different meanings . Similarly, BoW cannot distinguish between “I am happy” and “I am not happy” since it doesn’t capture negations or syntactic relationships .</p> <h4 id="semantic-limitations"><strong>Semantic Limitations</strong></h4> <p>BoW operates at a superficial lexical level without capturing deeper semantic relationships:</p> <ul> <li> <strong>Polysemy</strong>: Words with multiple meanings (like “bat” as a sports equipment or animal) are collapsed into a single representation</li> <li> <strong>Synonymy</strong>: Different words with similar meanings (like “scary” and “frightening”) are treated as completely distinct features</li> <li> <strong>Conceptual phrases</strong>: Multi-word expressions that form single semantic units (like “New York” or “artificial intelligence”) are broken down into individual components, losing their unified meaning</li> </ul> <h4 id="computational-considerations"><strong>Computational Considerations</strong></h4> <p>As vocabulary size increases, BoW vectors become <strong>high-dimensional</strong> and <strong>sparse</strong> (containing mostly zeros) . This sparsity can lead to computational inefficiency and the “curse of dimensionality” in machine learning models. For large datasets with extensive vocabularies, the resulting BoW representation may require significant memory and processing resources .</p> <h2 id="evolution-and-alternatives"><strong>Evolution and Alternatives</strong></h2> <h4 id="tf-idf-addressing-word-importance"><strong>TF-IDF: Addressing Word Importance</strong></h4> <p><strong>Term Frequency-Inverse Document Frequency</strong> (TF-IDF) emerged as an enhancement to basic BoW by addressing its limitation of treating all words equally . TF-IDF adjusts word weights by considering both:</p> <ul> <li> <strong>Term Frequency</strong>: How often a word appears in a specific document</li> <li> <strong>Inverse Document Frequency</strong>: How rare the word is across the entire document collection</li> </ul> <p>This approach reduces the influence of common words that appear frequently across many documents while emphasizing words that are distinctive to particular documents . The comparison below highlights key differences:</p> <hr> <table> <thead> <tr> <th>Aspect</th> <th>Bag-of-Words (BoW)</th> <th>TF-IDF</th> </tr> </thead> <tbody> <tr> <td><strong>Word Importance</strong></td> <td>Treats all words equally</td> <td>Adjusts importance based on rarity</td> </tr> <tr> <td><strong>Handling Common Words</strong></td> <td>Common words can dominate representation</td> <td>Reduces weight of common words</td> </tr> <tr> <td><strong>Document Length Sensitivity</strong></td> <td>Highly sensitive to document length</td> <td>Normalizes for document length</td> </tr> <tr> <td><strong>Complexity</strong></td> <td>Simple and computationally inexpensive</td> <td>More complex due to IDF calculation</td> </tr> </tbody> </table> <hr> <h4 id="word-embeddings-and-deep-learning-approaches"><strong>Word Embeddings and Deep Learning Approaches</strong></h4> <p>More advanced techniques have emerged to address BoW’s limitations:</p> <ul> <li> <strong>Word2Vec</strong> (2013): Creates dense vector representations that capture semantic relationships between words based on their contextual usage</li> <li> <strong>GloVe</strong> (Global Vectors): Uses global word co-occurrence statistics to generate word embeddings</li> <li> <strong>FastText</strong>: Extends Word2Vec by representing words as bags of character n-grams, effectively handling out-of-vocabulary words</li> </ul> <h4 id="modern-transformer-models"><strong>Modern Transformer Models</strong></h4> <p>The field has evolved toward increasingly sophisticated architectures:</p> <ul> <li> <strong>BERT</strong> (2018): Bidirectional Transformer models that capture contextual word meanings based on surrounding text</li> <li> <strong>GPT series</strong>: Autoregressive models that generate human-like text by predicting subsequent words</li> <li> <strong>RoBERTa, T5, and others</strong>: Optimized variants that improve upon earlier transformer architectures</li> </ul> <p>These modern approaches represent a paradigm shift from the context-agnostic nature of BoW to models that capture rich contextual and semantic relationships .</p> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>Despite its simplicity and limitations, the Bag of Words model maintains <strong>enduring relevance</strong> in the NLP landscape. Its computational efficiency, interpretability, and effectiveness for specific tasks ensure its continued utility, particularly for:</p> <ul> <li> <strong>Baseline models</strong>: Providing a performance benchmark for more complex algorithms</li> <li> <strong>Resource-constrained environments</strong>: Offering a lightweight solution when computational resources are limited</li> <li> <strong>Specific applications</strong>: Remaining effective for tasks where word presence alone provides strong signals, such as spam detection and topic classification</li> </ul> <p>The evolution from Bag of Words to modern transformer models illustrates the iterative nature of technological progress in natural language processing. While contemporary approaches have undoubtedly surpassed BoW in capturing linguistic nuance and context, they build upon the fundamental intuition behind BoW: that statistical patterns of word distribution contain meaningful information about document content .</p> <p>As we continue to develop increasingly sophisticated language models, the Bag of Words approach remains a critical milestone in our understanding of how machines can process and analyze human language. Its legacy endures not only in specific applications but in the foundational principles it established for text representation in computational systems.</p> <h2 id="references"><strong>References</strong></h2> <ol> <li><a href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="external nofollow noopener" target="_blank">Bag-of-words model</a></li> <li><a href="https://builtin.com/machine-learning/bag-of-words" rel="external nofollow noopener" target="_blank">Bag-of-Words Model in NLP Explained</a></li> <li><a href="https://www.geeksforgeeks.org/nlp/bag-of-words-vs-tf-idf/" rel="external nofollow noopener" target="_blank">Bag-of-words vs TF-IDF</a></li> <li><a href="https://aiml.com/what-are-some-use-cases-of-bag-of-words-model/" rel="external nofollow noopener" target="_blank">Use cases of Bag of Words model</a></li> <li><a href="https://www.byteplus.com/en/topic/400438" rel="external nofollow noopener" target="_blank">The Origins of Bag of Words</a></li> <li><a href="https://www.geeksforgeeks.org/nlp/bag-of-words-bow-model-in-nlp/" rel="external nofollow noopener" target="_blank">Bag of words (BoW) model in NLP</a></li> <li><a href="https://medium.com/@nagvekar/bag-of-words-simplified-a-hands-on-guide-with-code-advantages-and-limitations-in-nlp-f47461873068" rel="external nofollow noopener" target="_blank">Medium: Bag of Words Simplified</a></li> <li><a href="https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/" rel="external nofollow noopener" target="_blank">Introduction to Bag-of-Words and TF-IDF</a></li> <li><a href="https://www.ibm.com/think/topics/bag-of-words" rel="external nofollow noopener" target="_blank">What is bag of words?</a></li> <li><a href="https://medium.com/nlplanet/a-brief-timeline-of-nlp-from-bag-of-words-to-the-transformer-family-7caad8bbba56" rel="external nofollow noopener" target="_blank">A Brief Timeline of NLP</a></li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <d-article> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/distance-measure/">Distance Measures for Data Science</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/bag-of-words-2/">Practical Applications of the Bag of Words Model</a> </li> <br> <br> </d-article> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Qirui(Micheli) Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/overrides.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>