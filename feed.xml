<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://micheliliuv87.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://micheliliuv87.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-28T02:30:24+00:00</updated><id>https://micheliliuv87.github.io/feed.xml</id><title type="html">blank</title><subtitle>Webpage containing all my academic activities, publications, projects, and blog posts. </subtitle><entry><title type="html">Example Applications of Bag of Words</title><link href="https://micheliliuv87.github.io/blog/2025/bag-of-words-2/" rel="alternate" type="text/html" title="Example Applications of Bag of Words"/><published>2025-01-05T21:30:21+00:00</published><updated>2025-01-05T21:30:21+00:00</updated><id>https://micheliliuv87.github.io/blog/2025/bag-of-words-2</id><content type="html" xml:base="https://micheliliuv87.github.io/blog/2025/bag-of-words-2/"><![CDATA[<h1 id="practical-applications-of-the-bag-of-words-model"><strong>Practical Applications of the Bag of Words Model</strong></h1> <h2 id="introduction--recap"><strong>Introduction &amp; Recap</strong></h2> <p>In our previous post, we explored the <strong>theoretical foundations</strong> of the Bag of Words (BoW) model—how it converts unstructured text into numerical representations by counting word frequencies while disregarding word order and grammar. This fundamental technique has proven remarkably effective across various <strong>Natural Language Processing (NLP) tasks</strong> despite its simplicity .</p> <p>Now, let’s transition from theory to practice. In this hands-on guide, we’ll implement the Bag of Words model for several real-world applications including text classification, sentiment analysis, and TF-IDF visualization. We’ll work with Python libraries like <code class="language-plaintext highlighter-rouge">scikit-learn</code>, <code class="language-plaintext highlighter-rouge">NLTK</code>, and <code class="language-plaintext highlighter-rouge">matplotlib</code> to build working examples you can adapt for your own projects.</p> <h2 id="text-classification-with-bow-spam-detection-example"><strong>Text Classification with BoW: Spam Detection Example</strong></h2> <p>One of the most common applications of Bag of Words is <strong>text classification</strong>, where we categorize documents into predefined classes. Let’s build a spam detection system that classifies messages as “spam” or “not spam.”</p> <h4 id="step-by-step-implementation"><strong>Step-by-Step Implementation</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">re</span>
<span class="kn">import</span> <span class="n">nltk</span>
<span class="kn">from</span> <span class="n">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="n">nltk.stem</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>
<span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># download NLTK sample data
</span><span class="n">nltk</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="sh">'</span><span class="s">stopwords</span><span class="sh">'</span><span class="p">)</span>
<span class="n">nltk</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="sh">'</span><span class="s">punkt</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># sample dataset (in practice, you'd use a larger dataset)
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">Win a free iPhone now! Click here to claim your prize.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Your package has been delivered. Track your shipment.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Congratulations! You won a $1000 gift card. Reply to claim.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Meeting scheduled for tomorrow at 10 AM in conference room.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Urgent! Your account needs verification. Update immediately.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">The quarterly report is attached for your review.</span><span class="sh">'</span>
    <span class="p">],</span>
    <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">not spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">not spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">not spam</span><span class="sh">'</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># text preprocessing function
</span><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># remove non-alphabetic characters and convert to lowercase
</span>    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">[^A-Za-z]</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>

    <span class="c1">#tokenize
</span>    <span class="n">words</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="c1">#remove stopwords
</span>    <span class="n">stop_words</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">))</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>

    <span class="c1">#apply stemming
</span>    <span class="n">stemmer</span> <span class="o">=</span> <span class="nc">PorterStemmer</span><span class="p">()</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="p">.</span><span class="nf">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>

    <span class="k">return</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="c1"># apply preprocessing
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">processed_text</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span><span class="n">preprocess_text</span><span class="p">)</span>

<span class="c1"># create Bag of Words representation
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">processed_text</span><span class="sh">'</span><span class="p">]).</span><span class="nf">toarray</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># split data into training and testing sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># train a Naive Bayes classifier
</span><span class="n">classifier</span> <span class="o">=</span> <span class="nc">GaussianNB</span><span class="p">()</span>
<span class="n">classifier</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># make predictions and evaluate accuracy
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Model Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h4 id="how-bow-helps-in-classification"><strong>How BoW Helps in Classification</strong></h4> <p>In this spam detection example, the Bag of Words model identifies <strong>characteristic word patterns</strong> in spam versus legitimate messages . Spam messages often contain words like “win,” “free,” “prize,” and “urgent,” while legitimate messages use more neutral language. By converting these word patterns into numerical features, we enable machine learning algorithms to learn the distinguishing characteristics of each category.</p> <p>The <code class="language-plaintext highlighter-rouge">CountVectorizer</code> from scikit-learn handles the heavy lifting of creating our BoW representation . The <code class="language-plaintext highlighter-rouge">max_features</code> parameter ensures we only consider the top 1000 most frequent words, preventing excessively high-dimensional data.</p> <h2 id="sentiment-analysis-with-bow"><strong>Sentiment Analysis with BoW</strong></h2> <p>Another powerful application of Bag of Words is <strong>sentiment analysis</strong>—determining whether a piece of text expresses positive, negative, or neutral sentiment. Let’s analyze movie reviews.</p> <h4 id="implementation-code"><strong>Implementation Code</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Sample movie reviews dataset
</span><span class="n">reviews</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">This movie was absolutely fantastic! Great acting and plot.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Terrible waste of time. Poor acting and boring story.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Loved the cinematography and character development.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">The worst movie I have ever seen in my life.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Brilliant performance by the lead actor. Highly recommended.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Mediocre at best. Nothing special about this film.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">An outstanding masterpiece that kept me engaged throughout.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Poor direction and weak screenplay. Very disappointing.</span><span class="sh">'</span>
<span class="p">]</span>

<span class="n">sentiments</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">positive</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">positive</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">positive</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">positive</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># Create BoW model with unigrams and bigrams
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">reviews</span><span class="p">)</span>

<span class="c1"># Train a classifier
</span><span class="n">classifier</span> <span class="o">=</span> <span class="nc">MultinomialNB</span><span class="p">()</span>
<span class="n">classifier</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">sentiments</span><span class="p">)</span>

<span class="c1"># Test with new reviews
</span><span class="n">test_reviews</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">The acting was good but the story was weak</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Amazing movie with fantastic performances</span><span class="sh">'</span>
<span class="p">]</span>

<span class="n">test_vectors</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">test_reviews</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">test_vectors</span><span class="p">)</span>

<span class="k">for</span> <span class="n">review</span><span class="p">,</span> <span class="n">sentiment</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">test_reviews</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Review: </span><span class="sh">'</span><span class="si">{</span><span class="n">review</span><span class="si">}</span><span class="sh">'</span><span class="s"> -&gt; Sentiment: </span><span class="si">{</span><span class="n">sentiment</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h4 id="understanding-the-output"><strong>Understanding the Output</strong></h4> <p>This example introduces an important enhancement: <strong>ngrams</strong> . By setting <code class="language-plaintext highlighter-rouge">ngram_range=(1, 2)</code>, we consider both single words (unigrams) and pairs of consecutive words (bigrams). This helps capture phrases like “absolutely fantastic” or “poor acting” that carry more nuanced sentiment than individual words.</p> <p>The <code class="language-plaintext highlighter-rouge">MultinomialNB</code> classifier is particularly well-suited for text classification with discrete features like word counts . It efficiently learns the probability distributions of words for each sentiment class.</p> <h2 id="tf-idf-enhancing-bag-of-words"><strong>TF-IDF: Enhancing Bag of Words</strong></h2> <p>While simple word counts are useful, they have a limitation: common words that appear frequently across all documents may dominate the analysis. <strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong> addresses this by weighting words based on their importance .</p> <h4 id="tf-idf-calculation-and-visualization"><strong>TF-IDF Calculation and Visualization</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># sample documents for TF-IDF demonstration
</span><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">Natural language processing with Python is fascinating and powerful</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Python is a great programming language for data science</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">I enjoy learning new things about NLP and Python programming</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Machine learning and artificial intelligence are changing the world</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Deep learning models require extensive computational resources</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Data scientists use Python for machine learning projects</span><span class="sh">"</span>
<span class="p">]</span>

<span class="c1"># calculate TF-IDF
</span><span class="n">tfidf_vectorizer</span> <span class="o">=</span> <span class="nc">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">)</span>
<span class="n">tfidf_matrix</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

<span class="c1"># get feature names and TF-IDF scores
</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">()</span>
<span class="n">dense_matrix</span> <span class="o">=</span> <span class="n">tfidf_matrix</span><span class="p">.</span><span class="nf">todense</span><span class="p">()</span>

<span class="c1"># display TF-IDF scores for first document
</span><span class="n">first_doc</span> <span class="o">=</span> <span class="n">dense_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">word_scores</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">first_doc</span><span class="p">),</span>
                    <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Top terms in first document:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">word_scores</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1">#dimension reduction for visualization
</span><span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca_result</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">dense_matrix</span><span class="p">)</span>

<span class="c1"># create visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">pca_result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">pca_result</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="c1">#ddd document labels
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([</span><span class="sa">f</span><span class="sh">"</span><span class="s">Doc </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">documents</span><span class="p">))]):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">pca_result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">pca_result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                 <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="sh">'</span><span class="s">offset points</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">TF-IDF Document Visualization using PCA</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Principal Component 1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Principal Component 2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h4 id="tf-idf-intuition"><strong>TF-IDF Intuition</strong></h4> <p>TF-IDF balances two factors :</p> <ul> <li><strong>Term Frequency (TF)</strong>: How often a word appears in a specific document</li> <li><strong>Inverse Document Frequency (IDF)</strong>: How rare the word is across all documents</li> </ul> <p>The product of these two values gives higher weight to words that are frequent in a specific document but rare in the overall collection. This effectively identifies words that are characteristic of each document.</p> <p><img src="https://i.ytimg.com/vi/zLMEnNbdh4Q/maxresdefault.jpg" alt="TF-IDF Visualization" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"/></p> <p><em>TF-IDF helps identify the most distinctive words in documents, pushing similar documents closer together in vector space .</em></p> <h2 id="advanced-techniques--best-practices"><strong>Advanced Techniques &amp; Best Practices</strong></h2> <h4 id="feature-engineering-with-n-grams"><strong>Feature Engineering with N-grams</strong></h4> <p>As mentioned in the sentiment analysis example, n-grams can significantly improve model performance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># comparing different n-gram ranges
</span><span class="n">unigram_vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">bigram_vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">trigram_vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># each approach captures different linguistic patterns
</span></code></pre></div></div> <p>Bigrams and trigrams help preserve contextual information that single words might lose . For example, the phrase “not good” has a very different meaning than the individual words “not” and “good” considered separately.</p> <h4 id="handling-large-vocabularies-with-feature-hashing"><strong>Handling Large Vocabularies with Feature Hashing</strong></h4> <p>For extremely large datasets, the BoW representation can become computationally challenging. <strong>Feature hashing</strong> (or the “hash trick”) provides a memory-efficient alternative :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">HashingVectorizer</span>

<span class="c1"># using hashing vectorizer for large datasets
</span><span class="n">hash_vectorizer</span> <span class="o">=</span> <span class="nc">HashingVectorizer</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">alternate_sign</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">X_hash</span> <span class="o">=</span> <span class="n">hash_vectorizer</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre></div></div> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>The Bag of Words model, while simple, remains remarkably powerful for practical text analysis tasks. Through our implementations, we’ve seen how BoW enables:</p> <ul> <li><strong>Text classification</strong> by identifying characteristic word patterns in different categories</li> <li><strong>Sentiment analysis</strong> by capturing emotional language through word frequencies</li> <li><strong>Enhanced analysis with TF-IDF</strong> by weighting words based on their distinctiveness</li> </ul> <p>The true power of Bag of Words lies in its <strong>versatility and interpretability</strong>. Unlike more complex deep learning models, BoW features are easily understandable and can provide valuable insights into what the model is learning.</p> <p>While modern approaches like word embeddings and transformer models have their place for complex NLP tasks, Bag of Words remains an excellent starting point for most text analysis projects—offering a compelling balance of simplicity, interpretability, and effectiveness.</p> <p><em>All code examples in this post are designed to be runnable with Python 3.6+ and standard data science libraries (pandas, scikit-learn, NLTK, matplotlib).</em></p> <hr/> <p><em>This practical guide builds upon the theoretical foundations established in our previous post about the Bag of Words model. Implement these techniques as a starting point for your text analysis projects, then experiment with different preprocessing approaches and parameter tuning to optimize for your specific use cases.</em></p> <h2 id="references"><strong>References</strong>:</h2> <ol> <li><a href="https://www.datacamp.com/tutorial/python-bag-of-words-model">Python Bag of Words Models</a></li> <li><a href="https://www.geeksforgeeks.org/machine-learning/visualizing-tf-idf-scores-a-comprehensive-guide-to-plotting-a-document-tf-idf-2d-graph/">Visualizing TF-IDF Scores: A Comprehensive Guide to Plotting a Document TF-IDF 2D Graph</a></li> <li><a href="https://medium.com/swlh/text-classification-using-the-bag-of-words-approach-with-nltk-and-scikit-learn-9a731e5c4e2f">Text classification using the Bag Of Words Approach with NLTK and Scikit Learn</a></li> <li><a href="https://medium.com/@GeoffreyGordonAshbrook/vector-visualization-2d-plot-your-tf-idf-with-pca-83fa9fccb1d">Vector Visualization: 2D Plot your TF-IDF with PCA</a></li> <li><a href="https://www.geeksforgeeks.org/nlp/bag-of-words-bow-model-in-nlp/">Bag of words (BoW) model in NLP</a></li> <li><a href="https://www.analyticsvidhya.com/blog/2021/08/a-friendly-guide-to-nlp-bag-of-words-with-python-example/">A friendly guide to NLP: Bag-of-Words with Python example</a></li> <li><a href="https://www.tidytextmining.com/tfidf">3 Analyzing word and document frequency: tf-idf</a></li> <li><a href="https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf">Analyzing Documents with TF-IDF</a></li> <li><a href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/">A Gentle Introduction to the Bag-of-Words Model</a></li> <li><a href="https://blog.quantinsti.com/bag-of-words/">Bag of Words: Approach, Python Code, Limitations</a></li> </ol>]]></content><author><name></name></author><category term="Text-processing"/><category term="BoW"/><summary type="html"><![CDATA[Application of Bag of words, examples of TF-IDF, Text Classification, and Sentiment Analysis.]]></summary></entry><entry><title type="html">An Introduction to Bag of Words</title><link href="https://micheliliuv87.github.io/blog/2025/bag-of-words-1/" rel="alternate" type="text/html" title="An Introduction to Bag of Words"/><published>2025-01-02T16:40:16+00:00</published><updated>2025-01-02T16:40:16+00:00</updated><id>https://micheliliuv87.github.io/blog/2025/bag-of-words-1</id><content type="html" xml:base="https://micheliliuv87.github.io/blog/2025/bag-of-words-1/"><![CDATA[<hr/> <h1 id="the-bag-of-words-model-a-comprehensive-analysis-of-nlps-foundational-technique"><strong>The Bag of Words Model: A Comprehensive Analysis of NLP’s Foundational Technique</strong></h1> <h2 id="introduction-to-bow"><strong>Introduction to BoW</strong></h2> <p>In the landscape of <strong>Natural Language Processing</strong> (NLP), few models have been as fundamentally important and enduringly influential as the <strong>Bag of Words</strong> (<a href="https://en.wikipedia.org/wiki/Bag-of-words_model">BoW</a>) model. At its core, BoW represents a straightforward yet powerful approach to text representation: it transforms unstructured text into a structured, numerical format by treating a document as an unordered collection of words while tracking their frequency. This conceptual simplicity has made BoW a <strong>foundational technique</strong> that continues to serve as a baseline for text classification and feature extraction tasks, despite the emergence of more sophisticated alternatives.</p> <p>The BoW model operates on a fundamental assumption that the frequency of words in a document captures meaningful information about its content, regardless of their order or grammatical relationships . This approach might seem counterintuitive—after all, human language relies heavily on word order and syntax for meaning—yet BoW has proven remarkably effective for many <a href="https://builtin.com/machine-learning/bag-of-words">practical NLP applications</a>, from spam detection to sentiment analysis.</p> <h2 id="history-in-short"><strong>History in Short</strong></h2> <p>The conceptual origins of Bag of Words trace back to the mid-20th century, with early references found in Zellig Harris’s 1954 article on “Distributional Structure” . The model emerged from the intersection of <strong>computational linguistics</strong> and <strong>information retrieval</strong> during the 1950s, when researchers sought pragmatic solutions for processing text data without needing to understand complex grammatical structures.</p> <p>Initially developed in the context of document classification and early information retrieval systems, BoW gained significant traction as researchers recognized that <strong>word frequency patterns</strong> could provide substantial insights into document content and categorization . By the 1990s, BoW had become a standard technique in natural language processing, finding robust applications in spam filtering, document classification, and early sentiment analysis systems.</p> <p>The mathematical foundation of BoW—representing documents as vectors where each dimension corresponds to a unique word and the value represents its frequency—revolutionized text analysis by enabling computational systems to perform mathematical operations on textual data . This transformation from unstructured text to structured numerical representation opened new possibilities for machine learning applications in natural language.</p> <h2 id="how-bag-of-words-works"><strong>How Bag of Words Works?</strong></h2> <h4 id="central-mechanism"><strong>Central Mechanism</strong></h4> <p>The Bag of Words model transforms text through a systematic process that disregards word order and syntax while preserving information about word occurrence and frequency . The standard implementation involves three key steps:</p> <ol> <li><strong>Tokenization</strong>: Breaking down text into individual words or tokens</li> <li><strong>Vocabulary Creation</strong>: Building a unique dictionary of all distinct words across the corpus</li> <li><strong>Vectorization</strong>: Converting each document into a numerical vector based on word frequencies</li> </ol> <h4 id="practical-implementation"><strong>Practical Implementation</strong></h4> <p>Here’s a concrete example that illustrates the BoW process:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="c1"># sample documents
</span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">I love programming and programming is fun</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">I love machine learning</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Machine learning is fascinating</span><span class="sh">'</span>
<span class="p">]</span>

<span class="c1"># build and fit vectorizer
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="c1"># print
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Vocabulary:</span><span class="sh">"</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">BoW matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="nf">toarray</span><span class="p">())</span>
</code></pre></div></div> <p>You would expect the following output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Vocabulary: ['and' 'fascinating' 'fun' 'is' 'learning' 'love' 'machine' 'programming']
BoW matrix:
 [[1 0 1 1 0 1 0 2]
 [0 0 0 0 1 1 1 0]
 [0 1 0 1 1 0 1 0]]
</code></pre></div></div> <p>The resulting matrix represents each document as a vector where each element corresponds to the frequency of a specific word from the vocabulary . The first document, for instance, contains the word “and” once, “fun” once, “is” once, “love” once, and “programming” twice.</p> <p>The BoW model creates what can be described as a <strong>vector space</strong> where each unique word becomes a separate dimension . Documents are then plotted as points in this multi-dimensional space, with their positions along each dimension determined by the frequency of the corresponding word. This representation enables mathematical comparison and analysis of documents based on their word distribution patterns.</p> <h2 id="applications-and-use-cases"><strong>Applications and Use Cases</strong></h2> <p>The Bag of Words model has found diverse applications across numerous domains of text analysis and machine learning:</p> <h4 id="text-classification"><strong>Text Classification</strong></h4> <p>BoW serves as a fundamental feature extraction technique for <strong>document categorization</strong> tasks. Email services extensively use BoW for spam detection by analyzing the frequency of specific words indicative of spam content . Similarly, news organizations employ BoW for <strong>topic classification</strong>, automatically categorizing articles based on their predominant vocabulary .</p> <h4 id="sentiment-analysis"><strong>Sentiment Analysis</strong></h4> <p>Companies leverage BoW to understand <strong>customer sentiment</strong> across reviews, social media, and feedback platforms. By mapping word frequencies to positive or negative sentiment indicators, businesses can gauge public opinion about products or services at scale . For instance, words like “awful” or “terrible” appearing frequently in product reviews strongly indicate negative sentiment, while “excellent” or “amazing” suggest positive experiences .</p> <h4 id="information-retrieval"><strong>Information Retrieval</strong></h4> <p>Early search engines relied heavily on BoW principles to match user queries with relevant documents . While modern search algorithms have incorporated more sophisticated techniques, the fundamental approach of measuring <strong>word frequency</strong> and <strong>presence</strong> remains crucial in information retrieval systems.</p> <h4 id="other-applications"><strong>Other Applications</strong></h4> <ul> <li><strong>Document similarity detection</strong>: Identifying similar documents based on shared word distribution patterns</li> <li><strong>Language identification</strong>: Determining the language of a document based on characteristic vocabulary</li> <li><strong>Recommendation systems</strong>: Analyzing product descriptions or user reviews to generate personalized recommendations</li> <li><strong>Text clustering</strong>: Grouping similar documents together without predefined categories</li> </ul> <h2 id="limitations-and-challenges"><strong>Limitations and Challenges</strong></h2> <p>Despite its widespread adoption and utility, the Bag of Words model faces several significant limitations:</p> <h4 id="contextual-understanding"><strong>Contextual Understanding</strong></h4> <p>The most notable drawback of BoW is its <strong>complete disregard for word order</strong> and contextual relationships . This limitation means that sentences with identical words but different meanings receive identical representations. For example, “Man bites dog” and “Dog bites man” are treated as the same by BoW, despite their dramatically different meanings . Similarly, BoW cannot distinguish between “I am happy” and “I am not happy” since it doesn’t capture negations or syntactic relationships .</p> <h4 id="semantic-limitations"><strong>Semantic Limitations</strong></h4> <p>BoW operates at a superficial lexical level without capturing deeper semantic relationships:</p> <ul> <li><strong>Polysemy</strong>: Words with multiple meanings (like “bat” as a sports equipment or animal) are collapsed into a single representation</li> <li><strong>Synonymy</strong>: Different words with similar meanings (like “scary” and “frightening”) are treated as completely distinct features</li> <li><strong>Conceptual phrases</strong>: Multi-word expressions that form single semantic units (like “New York” or “artificial intelligence”) are broken down into individual components, losing their unified meaning</li> </ul> <h4 id="computational-considerations"><strong>Computational Considerations</strong></h4> <p>As vocabulary size increases, BoW vectors become <strong>high-dimensional</strong> and <strong>sparse</strong> (containing mostly zeros) . This sparsity can lead to computational inefficiency and the “curse of dimensionality” in machine learning models. For large datasets with extensive vocabularies, the resulting BoW representation may require significant memory and processing resources .</p> <h2 id="evolution-and-alternatives"><strong>Evolution and Alternatives</strong></h2> <h4 id="tf-idf-addressing-word-importance"><strong>TF-IDF: Addressing Word Importance</strong></h4> <p><strong>Term Frequency-Inverse Document Frequency</strong> (TF-IDF) emerged as an enhancement to basic BoW by addressing its limitation of treating all words equally . TF-IDF adjusts word weights by considering both:</p> <ul> <li><strong>Term Frequency</strong>: How often a word appears in a specific document</li> <li><strong>Inverse Document Frequency</strong>: How rare the word is across the entire document collection</li> </ul> <p>This approach reduces the influence of common words that appear frequently across many documents while emphasizing words that are distinctive to particular documents . The comparison below highlights key differences:</p> <table> <thead> <tr> <th>Aspect</th> <th>Bag-of-Words (BoW)</th> <th>TF-IDF</th> </tr> </thead> <tbody> <tr> <td><strong>Word Importance</strong></td> <td>Treats all words equally</td> <td>Adjusts importance based on rarity</td> </tr> <tr> <td><strong>Handling Common Words</strong></td> <td>Common words can dominate representation</td> <td>Reduces weight of common words</td> </tr> <tr> <td><strong>Document Length Sensitivity</strong></td> <td>Highly sensitive to document length</td> <td>Normalizes for document length</td> </tr> <tr> <td><strong>Complexity</strong></td> <td>Simple and computationally inexpensive</td> <td>More complex due to IDF calculation</td> </tr> </tbody> </table> <h4 id="word-embeddings-and-deep-learning-approaches"><strong>Word Embeddings and Deep Learning Approaches</strong></h4> <p>More advanced techniques have emerged to address BoW’s limitations:</p> <ul> <li><strong>Word2Vec</strong> (2013): Creates dense vector representations that capture semantic relationships between words based on their contextual usage</li> <li><strong>GloVe</strong> (Global Vectors): Uses global word co-occurrence statistics to generate word embeddings</li> <li><strong>FastText</strong>: Extends Word2Vec by representing words as bags of character n-grams, effectively handling out-of-vocabulary words</li> </ul> <h4 id="modern-transformer-models"><strong>Modern Transformer Models</strong></h4> <p>The field has evolved toward increasingly sophisticated architectures:</p> <ul> <li><strong>BERT</strong> (2018): Bidirectional Transformer models that capture contextual word meanings based on surrounding text</li> <li><strong>GPT series</strong>: Autoregressive models that generate human-like text by predicting subsequent words</li> <li><strong>RoBERTa, T5, and others</strong>: Optimized variants that improve upon earlier transformer architectures</li> </ul> <p>These modern approaches represent a paradigm shift from the context-agnostic nature of BoW to models that capture rich contextual and semantic relationships .</p> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>Despite its simplicity and limitations, the Bag of Words model maintains <strong>enduring relevance</strong> in the NLP landscape. Its computational efficiency, interpretability, and effectiveness for specific tasks ensure its continued utility, particularly for:</p> <ul> <li><strong>Baseline models</strong>: Providing a performance benchmark for more complex algorithms</li> <li><strong>Resource-constrained environments</strong>: Offering a lightweight solution when computational resources are limited</li> <li><strong>Specific applications</strong>: Remaining effective for tasks where word presence alone provides strong signals, such as spam detection and topic classification</li> </ul> <p>The evolution from Bag of Words to modern transformer models illustrates the iterative nature of technological progress in natural language processing. While contemporary approaches have undoubtedly surpassed BoW in capturing linguistic nuance and context, they build upon the fundamental intuition behind BoW: that statistical patterns of word distribution contain meaningful information about document content .</p> <p>As we continue to develop increasingly sophisticated language models, the Bag of Words approach remains a critical milestone in our understanding of how machines can process and analyze human language. Its legacy endures not only in specific applications but in the foundational principles it established for text representation in computational systems.</p> <h2 id="references"><strong>References</strong></h2> <ol> <li><a href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag-of-words model</a></li> <li><a href="https://builtin.com/machine-learning/bag-of-words">Bag-of-Words Model in NLP Explained</a></li> <li><a href="https://www.geeksforgeeks.org/nlp/bag-of-words-vs-tf-idf/">Bag-of-words vs TF-IDF</a></li> <li><a href="https://aiml.com/what-are-some-use-cases-of-bag-of-words-model/">Use cases of Bag of Words model</a></li> <li><a href="https://www.byteplus.com/en/topic/400438">The Origins of Bag of Words</a></li> <li><a href="https://www.geeksforgeeks.org/nlp/bag-of-words-bow-model-in-nlp/">Bag of words (BoW) model in NLP</a></li> <li><a href="https://medium.com/@nagvekar/bag-of-words-simplified-a-hands-on-guide-with-code-advantages-and-limitations-in-nlp-f47461873068">Medium: Bag of Words Simplified</a></li> <li><a href="https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/">Introduction to Bag-of-Words and TF-IDF</a></li> <li><a href="https://www.ibm.com/think/topics/bag-of-words">What is bag of words?</a></li> <li><a href="https://medium.com/nlplanet/a-brief-timeline-of-nlp-from-bag-of-words-to-the-transformer-family-7caad8bbba56">A Brief Timeline of NLP</a></li> </ol>]]></content><author><name></name></author><category term="Text-processing"/><category term="BoW"/><summary type="html"><![CDATA[BoW introduction, history, and use cases]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://micheliliuv87.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://micheliliuv87.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://micheliliuv87.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://micheliliuv87.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://micheliliuv87.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://micheliliuv87.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website! 🎉🎉</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as sources.</p> <p>Any questions or suggestions? 👉 Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>