<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://micheliliuv87.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://micheliliuv87.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-29T01:14:29+00:00</updated><id>https://micheliliuv87.github.io/feed.xml</id><title type="html">blank</title><subtitle>Webpage containing all my academic activities, publications, projects, and blog posts. </subtitle><entry><title type="html">Distance Measures for Data Science</title><link href="https://micheliliuv87.github.io/blog/2025/distance-measure/" rel="alternate" type="text/html" title="Distance Measures for Data Science"/><published>2025-10-28T01:46:22+00:00</published><updated>2025-10-28T01:46:22+00:00</updated><id>https://micheliliuv87.github.io/blog/2025/distance-measure</id><content type="html" xml:base="https://micheliliuv87.github.io/blog/2025/distance-measure/"><![CDATA[<h3 id="notes-for-readers"><strong>Notes for Readers:</strong></h3> <ol> <li> <p><strong>Foundational for Data Science:</strong> These distance measures are the bedrock of many fundamental algorithms. You will use them in:</p> <ul> <li><strong>Clustering</strong> (e.g., K-Means, Hierarchical Clustering)</li> <li><strong>Classification</strong> (e.g., K-Nearest Neighbors)</li> <li><strong>Anomaly Detection</strong> (to find outliers)</li> <li><strong>Recommendation Systems</strong> (to find similar users or items)</li> <li><strong>Dimensionality Reduction</strong> (e.g., in the core of MDS or t-SNE)</li> </ul> </li> <li> <p><strong>Ubiquitous in Practice:</strong> The distances covered here include notes and hands-on practices you can try on your own. You will likely encounter them repeatedly in coursework, projects, and real-world applications.</p> </li> <li> <p><strong>A Starting Point, Not the Finish Line:</strong> This collection is not exhaustive (it omits, for example, Hamming distance, Jaccard index, and Earth Mover’s Distance), but it covers the most distances that are at least used once or multiple times in my school and work projects. So it is a good starting point to begin and learn.</p> </li> </ol> <h1 id="cosine-type-distance"><strong>Cosine Type Distance</strong></h1> <h2 id="cosine-distance"><strong>Cosine Distance</strong></h2> <p>(Almost same direction in high dimentional vectors, and similar angle)</p> <p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20200911171455/UntitledDiagram2.png" alt="Cosine Similarity" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"/></p> <h4 id="definition"><strong>Definition</strong></h4> <p>For vectors $\mathbf{s}$ and $\mathbf{t}$ in d-dimensional space, the <strong>cosine similarity</strong> is defined as:</p> \[\cos(\mathbf{s}, \mathbf{t}) = \frac{\mathbf{s}^\mathsf{T}\mathbf{t}}{\|\mathbf{s}\|_2 \, \|\mathbf{t}\|_2} = \frac{\sum_{j=1}^{d} s_j t_j}{\sqrt{\sum_{j=1}^{d} s_j^2} \cdot \sqrt{\sum_{j=1}^{d} t_j^2}}\] <p>Then the <strong>cosine distance</strong> is:</p> \[d(\mathbf{s}, \mathbf{t}) = 1 - \cos(\mathbf{s}, \mathbf{t})\] <ul> <li>Value Range: Cosine similarity ranges between $[-1, +1]$ <strong>[−1 (perfectly dissimilar) and +1 (perfectly similar)]</strong>. If two vectors have the same direction, similarity $= +1$, distance $= 0$; if they have opposite directions, similarity $= -1$, distance $= 2$(though -1 is less common in many real-world datasets if all values are non-negative).</li> </ul> <h4 id="why-and-how-its-used"><strong>Why and How It’s Used</strong></h4> <ol> <li>Focuses on Orientation, Not Magnitude <ul> <li>Cosine measures the angle between vectors, independent of their magnitudes.</li> <li>Even if vectors differ in overall scale, the cosine similarity can remain high if they point in a similar direction.</li> </ul> </li> <li>Common in Text Analysis <ul> <li>In high-dimensional sparse vectors like (Bag-of-words) TF–IDF, cosine similarity is high if the directions are similar, regardless of absolute frequencies.</li> </ul> </li> <li>Computationally Simple <ul> <li>Only requires the dot product and vector norms, making it relatively efficient to compute.</li> </ul> </li> </ol> <h4 id="how-it-works-intuitive-understanding"><strong>How It Works (Intuitive Understanding)</strong></h4> <ul> <li>The more similar the directions of two vectors, the smaller the angle → the greater the cosine similarity → the smaller the cosine distance.</li> <li>If vectors are almost orthogonal or diverge, the cosine similarity is small → the cosine distance is large.</li> </ul> <h4 id="interpretation"><strong>Interpretation</strong></h4> <ul> <li>$vec_b$ is essentially $vec_a * 2$, so they point in the exact same direction.</li> <li>Cosine similarity is 1.0 → Cosine distance is 0.</li> </ul> <h4 id="example"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">cosine_distance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="n">dot_product</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">norm_s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="c1"># np.dot: try to write this out in your own practice because dot product is already vectorized.
</span>    <span class="n">norm_t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="c1"># Cosine similarity
</span>    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">dot_product</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm_s</span> <span class="o">*</span> <span class="n">norm_t</span><span class="p">)</span>

    <span class="c1"># Cosine distance
</span>    <span class="n">cos_dist</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cos_sim</span>
    <span class="k">return</span> <span class="n">cos_dist</span>

<span class="c1"># Example usage:
</span><span class="n">vec_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">vec_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>

<span class="n">dist</span> <span class="o">=</span> <span class="nf">cosine_distance</span><span class="p">(</span><span class="n">vec_a</span><span class="p">,</span> <span class="n">vec_b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Cosine Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Cosine Distance: 2.0
</code></pre></div></div> <h4 id="in-more-detail"><strong>In More Detail</strong></h4> <p><strong>Dot Product Part</strong> ($\mathbf{s}^\mathsf{T}\mathbf{t}$ is exactly the <strong>dot product</strong>: $\mathbf{s}^\mathsf{T}\mathbf{t} = \sum_{j=1}^{d} s_j t_j$)</p> <ul> <li>In English, “$\mathbf{s}^\mathsf{T}\mathbf{t}$” is exactly the dot product of vectors $\mathbf{s}$ and $\mathbf{t}$.</li> <li>In this formula, $\mathbf{s}^\mathsf{T}\mathbf{t}$ represents the sum of the coordinate-wise products of vectors $\mathbf{s}$ and $\mathbf{t}$, which is exactly what we call the vector dot product.</li> </ul> <p><strong>Vector Norm</strong> (This $|\mathbf{s}|_2$ and $|\mathbf{t}|_2$ are the <strong>Vector Norms</strong>)</p> <ul> <li>$|\mathbf{s}|_2$ is also called the L2 norm (Euclidean norm) of vector $\mathbf{s}$, which can be understood as the vector’s length or magnitude.</li> <li> <p>Mathematical definition:</p> \[\|\mathbf{s}\|_2 = \sqrt{\sum_{j=1}^{d} s_j^2}\] </li> <li>$|\mathbf{s}|_2$ represents the length of vector $\mathbf{s}$, calculated by squaring each coordinate, summing these squares, and then taking the square root.</li> </ul> <h4 id="example-1"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>

<span class="c1"># dot product
</span><span class="n">dot_value</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">s • t (dot product):</span><span class="sh">"</span><span class="p">,</span> <span class="n">dot_value</span><span class="p">)</span>

<span class="c1"># norm
</span><span class="n">norm_s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">norm_t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">||s||_2:</span><span class="sh">"</span><span class="p">,</span> <span class="n">norm_s</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">||t||_2:</span><span class="sh">"</span><span class="p">,</span> <span class="n">norm_t</span><span class="p">)</span>

<span class="c1"># cosine similarity
</span><span class="n">cos_sim</span> <span class="o">=</span> <span class="n">dot_value</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm_s</span> <span class="o">*</span> <span class="n">norm_t</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Cosine similarity:</span><span class="sh">"</span><span class="p">,</span> <span class="n">cos_sim</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>s • t (dot product): 32
||s||<span class="se">\_</span>2: 3.7416573867739413
||t||<span class="se">\_</span>2: 8.774964387392123
Cosine similarity: 0.9746318461970762
</code></pre></div></div> <hr/> <p><br/></p> <h2 id="pearson-correlation-distance"><strong>Pearson Correlation Distance</strong></h2> <p>(The first array appears to be derived by subtracting a constant from the second array)</p> <p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20250723175534566635/pearson_correlation_coefficient.webp" alt="Pearson Correlation" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"/></p> <h4 id="definition-1"><strong>Definition</strong></h4> <ul> <li>The Pearson correlation coefficient measures the <strong>linear</strong> relationship between two vectors.</li> <li>Sentence Structure: We first perform mean-centering on vectors $\mathbf{x}$ and $\mathbf{y}$, then compute their cosine similarity.</li> <li>English Terms: mean-centering, cosine similarity</li> </ul> <h4 id="mathematical-expression"><strong>Mathematical Expression</strong></h4> \[\mathrm{corr}(\mathbf{x}, \mathbf{y}) = \frac{\sum_{j=1}^{d} (x_j - \bar{x})(y_j - \bar{y})} {\sqrt{\sum_{j=1}^{d} (x_j - \bar{x})^2}\;\sqrt{\sum_{j=1}^{d} (y_j - \bar{y})^2}}\] <p>Where</p> \[\bar{x} = \frac{1}{d}\sum_{j=1}^{d} x_j\] <p>and,</p> \[\bar{y} = \frac{1}{d}\sum_{j=1}^{d} y_j\] <h4 id="pearson-correlation-distance-is-defined-as"><strong>Pearson Correlation Distance is defined as</strong>:</h4> \[d_{\text{Pearson}}(\mathbf{x}, \mathbf{y}) = 1 - \mathrm{corr}(\mathbf{x}, \mathbf{y})\] <h4 id="why-and-how-its-used-1"><strong>Why and How It’s Used</strong></h4> <ol> <li><strong>Measures Linear Association</strong> <ul> <li>Sentence Structure: When you want to know if two vectors have a proportional or inversely proportional linear relationship, you can use the Pearson correlation coefficient. <ul> <li>A smaller distance (correlation coefficient closer to 1) indicates higher similarity in linear variation across dimensions.</li> </ul> </li> <li>English Terms: linear relationship</li> </ul> </li> <li><strong>Insensitive to Scale and Offset</strong> <ul> <li>If one vector is just a linear scaling or shift of another, they can still have a high Pearson correlation coefficient.</li> </ul> </li> <li><strong>Applications in Data Analysis</strong> <ul> <li>Feature selection: Find features highly linearly associated with the target variable.</li> <li>Clustering: Group samples with similar linear variation patterns.</li> </ul> </li> </ol> <h4 id="how-it-works"><strong>How It Works</strong></h4> <ul> <li>First, subtract the mean from each vector (mean-centering).</li> <li>Then perform a calculation similar to cosine similarity, but using the “mean-centered” vectors.</li> <li>Finally, use <code class="language-plaintext highlighter-rouge">1 - corr</code> to get the distance value: higher correlation coefficient ⇒ smaller distance.</li> </ul> <h4 id="example-2"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">pearson_correlation_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># change to float better for calculation
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

    <span class="c1"># calculate mean
</span>    <span class="n">x_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># mean-centering
</span>    <span class="n">x_centered</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x_mean</span>
    <span class="n">y_centered</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_mean</span>

    <span class="c1"># dot product
</span>    <span class="n">dot_xy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x_centered</span><span class="p">,</span> <span class="n">y_centered</span><span class="p">)</span>

    <span class="c1"># norm
</span>    <span class="n">norm_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x_centered</span><span class="p">)</span>
    <span class="n">norm_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">y_centered</span><span class="p">)</span>

    <span class="c1"># If one vector is all constant(except prime numbers), norm will be 0, thus the correlation distance can be set to 1
</span>    <span class="k">if</span> <span class="n">norm_x</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">norm_y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1.0</span>

    <span class="c1"># Pearson correlation
</span>    <span class="n">pearson_corr</span> <span class="o">=</span> <span class="n">dot_xy</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm_x</span> <span class="o">*</span> <span class="n">norm_y</span><span class="p">)</span>

    <span class="c1"># Pearson distance
</span>    <span class="n">distance</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">pearson_corr</span>
    <span class="k">return</span> <span class="n">distance</span>

<span class="c1"># Sample data
</span><span class="n">vec_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>
<span class="n">vec_b</span> <span class="o">=</span> <span class="p">[</span> <span class="mi">5</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="n">dist</span> <span class="o">=</span> <span class="nf">pearson_correlation_distance</span><span class="p">(</span><span class="n">vec_a</span><span class="p">,</span> <span class="n">vec_b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Pearson Correlation Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist</span><span class="p">)</span>

<span class="c1"># Note: in this case, vec_b appears to be vec_a minus one constant (offsey ~5). If we are expecting they have higher correlation, the distance will be smaller.
</span></code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Pearson Correlation Distance: -2.220446049250313e-16
</code></pre></div></div> <hr/> <p><br/></p> <h2 id="spearman-correlation-distance-rank-corr"><strong>Spearman Correlation Distance (Rank Corr)</strong></h2> <p><img src="https://statistics.laerd.com/statistical-guides/img/spearman-1-small.png" alt="Spearman Correlation" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"/></p> <h4 id="definition-2"><strong>Definition</strong></h4> <ul> <li>The Spearman correlation coefficient measures the <strong>rank-based</strong> monotonic relationship between two vectors.</li> <li>Sentence Structure: First map the element values of each vector to their respective ranks within the vector, then perform operations similar to Pearson correlation on these “rank vectors.”</li> </ul> <h4 id="mathematical-expression-1"><strong>Mathematical Expression</strong></h4> <ul> <li>Similar to <a href="#pearson-correlation-distance-the-first-array-appears-to-be-derived-by-subtracting-a-constant-from-the-second-array">Pearson</a>, but instead of mean-centering vectors $\mathbf{x}$ and $\mathbf{y}$, first convert $\mathbf{x}$ to its rank vector, then convert $\mathbf{y}$ to its rank vector, and then calculate the Pearson correlation coefficient.</li> </ul> \[\mathrm{corr}_{\mathrm{Spearman}}(\mathbf{x}, \mathbf{y}) = \mathrm{corr}(\mathrm{rank}(\mathbf{x}),\, \mathrm{rank}(\mathbf{y}))\] <ul> <li>The corresponding <strong>Spearman Correlation Distance</strong> is then:</li> </ul> \[d_{\text{Spearman}}(\mathbf{x}, \mathbf{y}) = 1 - \mathrm{corr}_{\mathrm{Spearman}}(\mathbf{x}, \mathbf{y})\] <h4 id="why-and-how-its-used-2"><strong>Why and How It’s Used</strong></h4> <ol> <li><strong>Focuses on Ranking Rather Than Specific Values</strong> <ul> <li>Sentence Structure: As long as the order of values in two vectors is consistent (or nearly consistent), even if their value distributions aren’t strictly linear, they can still achieve a high Spearman correlation.</li> <li>English Terms: rank, monotonic</li> </ul> </li> <li><strong>More Robust to Outliers</strong> <ul> <li>Because it only considers relative size (which is larger/smaller) and not the absolute magnitude of differences, extreme points don’t affect it as much as they do in Pearson correlation.</li> </ul> </li> <li><strong>Used in Non-linear Monotonic Relationship Scenarios</strong> <ul> <li>For example, when one vector increases as the other increases (possibly in a curved relationship), Spearman can still capture this monotonic trend.</li> </ul> </li> </ol> <h4 id="how-it-works-1"><strong>How It Works</strong></h4> <ul> <li>Rank the elements of $\mathbf{x}$ and $\mathbf{y}$ from smallest to largest.</li> <li>After obtaining the rank vectors, compute the Pearson correlation, then take 1 - correlation to get the distance.</li> </ul> <h4 id="example-3"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">spearman_correlation_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>

    <span class="c1"># make ranks for x,y  (rank)
</span>    <span class="n">x_rank</span> <span class="o">=</span> <span class="nf">rank_vector</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y_rank</span> <span class="o">=</span> <span class="nf">rank_vector</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># calculate pearson correlation but with x_rank, y_rank
</span>    <span class="k">return</span> <span class="nf">pearson_correlation_distance</span><span class="p">(</span><span class="n">x_rank</span><span class="p">,</span> <span class="n">y_rank</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">rank_vector</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="c1"># order ascending by rank and give min rank=1，2nd min rank=2，and so on
</span>    <span class="n">sorted_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="n">ranks</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">sorted_idx</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ranks</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
    <span class="k">return</span> <span class="n">ranks</span>

<span class="k">def</span> <span class="nf">pearson_correlation_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># use the same Pearson equation
</span>    <span class="n">x_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">x_centered</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x_mean</span>
    <span class="n">y_centered</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_mean</span>

    <span class="n">dot_xy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x_centered</span><span class="p">,</span> <span class="n">y_centered</span><span class="p">)</span>
    <span class="n">norm_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x_centered</span><span class="p">)</span>
    <span class="n">norm_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">y_centered</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">norm_x</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">norm_y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1.0</span>
    <span class="n">corr</span> <span class="o">=</span> <span class="n">dot_xy</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm_x</span> <span class="o">*</span> <span class="n">norm_y</span><span class="p">)</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">corr</span>
    <span class="k">return</span> <span class="n">distance</span>

<span class="c1"># sample data
</span><span class="n">vec_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
<span class="n">vec_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="n">dist_spearman</span> <span class="o">=</span> <span class="nf">spearman_correlation_distance</span><span class="p">(</span><span class="n">vec_a</span><span class="p">,</span> <span class="n">vec_b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Spearman Correlation Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_spearman</span><span class="p">)</span>

<span class="c1">#	in vec_a, order is (10, 20, 100) When they rank，you get rank = (1, 2, 3).
#	in vec_b, order is (5, 10, 90) when they rank, you get rank = (1, 2, 3).
#	Therefore their rank is basically the same, so Spearman distance will be low
</span></code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Spearman Correlation Distance: 2.220446049250313e-16
</code></pre></div></div> <hr/> <p><br/></p> <p><br/></p> <h1 id="generalized-distance-metrics"><strong>Generalized Distance (Metrics)</strong></h1> <h2 id="minkowski-distance"><strong>Minkowski Distance</strong></h2> <p><img src="https://www.kdnuggets.com/wp-content/uploads/c_distance_metrics_euclidean_manhattan_minkowski_oh_12.jpg" alt="Minkowski Distance" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"/></p> <h4 id="definition-3"><strong>Definition</strong></h4> <ul> <li>For two points $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ and $\mathbf{y} = (y_1, y_2, \ldots, y_n)$ in n-dimensional space, the Minkowski distance is defined as:</li> </ul> \[d(\mathbf{x}, \mathbf{y}) = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{\frac{1}{p}}\] <h4 id="where-p-is-a-parameter-that-determines-the-type-of-distance"><strong>Where $p$ is a parameter that determines the type of distance:</strong></h4> <ul> <li>When $p = 1$, it yields the Manhattan Distance.</li> <li>When $p = 2$, it yields the Euclidean Distance.</li> <li>As long as $p \ge 1$, it is a valid Minkowski distance.</li> </ul> <h4 id="why-and-how-its-used-3"><strong>Why and How It’s Used</strong></h4> <ol> <li><em>Generalization</em>: Minkowski distance is a generalized form of many other distance metrics (like Euclidean, Manhattan).</li> <li><em>Flexibility</em>: You can choose different $p$ values based on requirements: <ul> <li>$p = 1$: Scenarios that emphasize absolute differences (like city block distance).</li> <li>$p = 2$: The most commonly used straight-line distance.</li> </ul> </li> <li><em>Machine Learning</em>: In algorithms like k-Nearest Neighbors (k-NN), you can try different $p$ values to see which distance metric performs better.</li> </ol> <h4 id="how-it-works-intuitive-understanding-1"><strong>How It Works (Intuitive Understanding)</strong></h4> <ul> <li>First calculate the absolute difference for each coordinate, raise it to the power of $p$, sum these values, and finally take the <strong>p-th root</strong>.</li> <li>When $p$ increases, larger differences have a greater impact on the result.</li> </ul> <h4 id="example-4"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">minkowski_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="c1"># x and y are lists or NumPy arrays
</span>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="n">p</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">p</span><span class="p">)</span>

<span class="c1"># Example usage:
</span><span class="n">point_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">point_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">dist_p1</span> <span class="o">=</span> <span class="nf">minkowski_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Manhattan Distance
</span><span class="n">dist_p2</span> <span class="o">=</span> <span class="nf">minkowski_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Euclidean Distance
</span><span class="n">dist_p3</span> <span class="o">=</span> <span class="nf">minkowski_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Minkowski Distance with p=1 (Manhattan):</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_p1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Minkowski Distance with p=2 (Euclidean):</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_p2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Minkowski Distance with p=3:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_p3</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Minkowski Distance with p=1 (Manhattan): 9.0
Minkowski Distance with p=2 (Euclidean): 5.385164807134504
Minkowski Distance with p=3: 4.626065009182741
</code></pre></div></div> <hr/> <p><br/></p> <h2 id="euclidean-distance"><strong>Euclidean Distance</strong></h2> <p>(Point it out, use very often in your data science projects)</p> <p><img src="https://rosalind.info/media/Euclidean_distance.png" alt="Euclidean Distance" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"/></p> <h4 id="definition-4"><strong>Definition</strong></h4> <ul> <li>For two points $\mathbf{x}$ and $\mathbf{y}$ in n-dimensional space, the Euclidean distance (also called L2 distance) is defined as:</li> </ul> \[d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}\] <p>It is a special case of the Minkowski distance when $p = 2$.</p> <h4 id="why-and-how-its-used-4">Why and How It’s Used</h4> <ol> <li><strong>Straight-line Distance</strong>: It represents the “straight-line” distance between points in Euclidean space.</li> <li><strong>Usage in Machine Learning</strong>: <ul> <li>In k-Nearest Neighbors (k-NN), it is used to measure similarity or dissimilarity between samples.</li> <li>In clustering (such as k-means), Euclidean distance is often used as a measure of cluster compactness.</li> </ul> </li> <li><strong>Geometric Meaning</strong>: It has a very clear geometric interpretation - the length of the line segment connecting the two points.</li> </ol> <h4 id="how-it-works-intuitive-understanding-2">How It Works (Intuitive Understanding)</h4> <ul> <li>Calculate the difference for each corresponding coordinate, square it, sum up all the squared differences, and then take the square root.</li> <li>If the differences are large, the distance will be large as well.</li> </ul> <h4 id="example-5"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span> <span class="c1">#this 1/2 can use sqrt to replace
</span>    <span class="c1">#return np.sum((x - y) ** 2) ** (1/2)
</span>
<span class="n">point_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">point_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">dist_euclidean</span> <span class="o">=</span> <span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Euclidean Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_euclidean</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Euclidean Distance: 5.385164807134504
</code></pre></div></div> <hr/> <p><br/></p> <h2 id="chebyshev-distance-max-abs-diff-in-coordinate-points"><strong>Chebyshev Distance (Max abs diff. in Coordinate points)</strong></h2> <p><img src="https://iq.opengenus.org/content/images/2018/12/chebyshev.png" alt="Chebyshev Distance" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"/></p> <h4 id="definition-5"><strong>Definition</strong></h4> <ul> <li>For two points $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ and $\mathbf{y} = (y_1, y_2, \ldots, y_n)$ in n-dimensional space, the Chebyshev Distance is defined as:</li> </ul> \[d_{\text{Chebyshev}}(\mathbf{x}, \mathbf{y}) = \max_{1 \le i \le n} \big| x_i - y_i \big|\] <p>Simply put, it is the maximum of the absolute differences between corresponding coordinates.</p> <h4 id="why-and-how-its-used-5"><strong>Why and How It’s Used</strong></h4> <ol> <li><strong>Maximum Difference</strong>: <ul> <li>Unlike other distances that “sum” or “average” differences across multiple coordinates, Chebyshev distance only focuses on the largest difference.</li> <li>If one coordinate has a particularly large difference while others have small differences, this single largest difference will determine the overall distance.</li> </ul> </li> <li><strong>Geometric Interpretation</strong>: <ul> <li>On a 2D grid (such as a chessboard), Chebyshev distance can be seen as the minimum number of moves a King needs to move from one square to another, since the King can move one step in any direction (including diagonally).</li> <li>In n-dimensional space, this means being able to move 1 unit simultaneously across multiple coordinates, similar to diagonal movement.</li> </ul> </li> <li><strong>Application Scenarios</strong>: <ul> <li>Chessboard or grid problems: Calculating the number of moves for a King.</li> <li>Computer games: When diagonal movement is allowed with the same cost as straight movement, Chebyshev distance can measure movement distance.</li> <li>Clustering or anomaly detection: Useful when we want to be highly sensitive to large deviations in any single feature (where a large difference in just one feature makes points distant).</li> </ul> </li> </ol> <h4 id="how-it-works-intuitive-understanding-3"><strong>How It Works (Intuitive Understanding)</strong></h4> <ul> <li>Calculate the absolute difference between corresponding coordinates of the two points: $\lvert x_i - y_i \rvert$.</li> <li>Find the maximum value among these differences.</li> <li>This maximum value is the Chebyshev distance.</li> </ul> <h4 id="example-6"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">chebyshev_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>

<span class="c1"># Example usage:
</span><span class="n">point_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">point_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="n">dist_chebyshev</span> <span class="o">=</span> <span class="nf">chebyshev_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Chebyshev Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_chebyshev</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chebyshev Distance: 6
</code></pre></div></div> <h4 id="comparison-between-euclidean-manhattan-chebyshev-distance"><strong>Comparison Between Euclidean/ Manhattan/ Chebyshev Distance</strong></h4> <p><img src="https://iq.opengenus.org/content/images/2018/12/distance.jpg" alt="Compare Three Distance" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"/></p> <hr/> <p><br/></p> <p><br/></p> <h1 id="scaled-weighted-distance"><strong>Scaled Weighted Distance</strong></h1> <h2 id="manhalanobis-distance-scaled-euclidean"><strong>Manhalanobis Distance (Scaled Euclidean)</strong></h2> <p>(Consider Feature Correlation)</p> <p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KzsugPQU-BTjvDACXbu9qw.jpeg" alt="Manhalanobis Distance1" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"/></p> <h4 id="definition-6"><strong>Definition</strong></h4> <ul> <li>For two vectors (points) $\mathbf{s}$ and $\mathbf{t}$ in d-dimensional space, the Mahalanobis Distance is defined as:</li> </ul> \[d(\mathbf{s}, \mathbf{t}) = \sqrt{(\mathbf{s} - \mathbf{t})^\mathsf{T} \, \mathbf{C}^{-1} \, (\mathbf{s} - \mathbf{t})}\] <p>where $\mathbf{C}$ is the covariance matrix of the data, and $\mathbf{C}^{-1}$ is its inverse matrix.</p> <h4 id="why-and-how-its-used-6"><strong>Why and How It’s Used</strong></h4> <ol> <li><strong>Accounts for Feature Correlation</strong>: <ul> <li>Unlike ordinary Euclidean distance, Mahalanobis distance considers correlations between different features.</li> <li>If two features are highly correlated, the distance in that direction gets “scaled down.”</li> </ul> </li> <li><strong>Scale Invariance</strong>: <ul> <li>Automatically adjusts for features with different scales based on covariance. If one feature has a particularly large numeric range, it won’t dominate the distance measure.</li> </ul> </li> <li><strong>Application Scenarios</strong>: <ul> <li>Outlier detection: Points that differ significantly from the data’s covariance structure are identified as outliers.</li> <li>Classification/Clustering: When dealing with strongly correlated features, using Mahalanobis distance can more accurately reflect true distance relationships.</li> </ul> </li> <li><strong>Covariates Matching</strong>: <ul> <li>This situation requires a reference subject.</li> <li>There’s a probability that covariates will skew the distance between subjects. <ul> <li>A good solution can be to use the rank of each covariate rather than its value.</li> <li>Or use trimmed mean/median instead of mean.</li> </ul> </li> </ul> </li> </ol> <h4 id="how-it-works-intuitive-understanding-4"><strong>How It Works (Intuitive Understanding)</strong></h4> <ul> <li>First calculate the difference between $\mathbf{s}$ and $\mathbf{t}$.</li> <li>Use the inverse covariance matrix $\mathbf{C}^{-1}$ to scale this difference, measuring its “degree of difference” in terms of data variance and correlation.</li> <li>Finally, take the square root to get the distance.</li> </ul> <h4 id="example-7"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">mahalanobis_distance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">cov</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    s, t: 1D arrays or lists representing points in d-dimensional space
    cov: covariance matrix (d x d)
    </span><span class="sh">"""</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">s</span> <span class="o">-</span> <span class="n">t</span>
    <span class="n">inv_cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>  <span class="c1"># Invert the covariance matrix
</span>    <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">diff</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">inv_cov</span> <span class="o">@</span> <span class="n">diff</span><span class="p">)</span> <span class="c1"># @ is the matrix multiplication operator
</span>    <span class="k">return</span> <span class="n">dist</span>

<span class="c1"># Example usage:
</span><span class="n">point_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="c1"># 2D point can also be np.array([[2, 3],[4,5]]) try yourself
</span><span class="n">point_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>

<span class="c1"># Suppose we know/have an estimated covariance matrix for our 2D data:
</span><span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                       <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>  <span class="c1"># Just an example sometimes you don't know
</span>
<span class="n">dist_mahalanobis</span> <span class="o">=</span> <span class="nf">mahalanobis_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">,</span> <span class="n">cov_matrix</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Mahalanobis Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_mahalanobis</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mahalanobis Distance: 2.8784916685156974
</code></pre></div></div> <h4 id="example-2-covariate-matching"><strong>Example 2: Covariate Matching</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">m_distnace</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span><span class="n">xj</span><span class="p">,</span><span class="n">cov</span><span class="p">):</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span>
    <span class="n">xj</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">xj</span><span class="p">)</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">xi</span> <span class="o">-</span> <span class="n">xj</span>
    <span class="n">inv_cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">cov</span><span class="p">)</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">diff</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">inv_cov</span> <span class="o">@</span> <span class="n">diff</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dist</span>

<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">335.285714</span><span class="p">,</span> <span class="mf">4.8095238</span><span class="p">,</span> <span class="mf">3.78571429</span><span class="p">],</span>
                <span class="p">[</span><span class="mf">4.80952381</span><span class="p">,</span> <span class="mf">0.23809524</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0238095</span><span class="p">],</span>
                <span class="p">[</span><span class="mf">3.78571429</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0238095</span><span class="p">,</span> <span class="mf">0.28571429</span><span class="p">]])</span>

<span class="c1"># The Treatment column is not a feature for distance computation because it represents a categorical assignment rather than a characteristic.
</span>
<span class="c1"># Data from the table (Age, College, Male) # covariance is assumed not fixed params
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">68</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>  <span class="c1"># Subject 1 (reference)
</span>    <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>  <span class="c1"># Subject 2
</span>    <span class="p">[</span><span class="mi">65</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>  <span class="c1"># Subject 3
</span>    <span class="p">[</span><span class="mi">76</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>  <span class="c1"># Subject 4
</span>    <span class="p">[</span><span class="mi">44</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>  <span class="c1"># Subject 5
</span>    <span class="p">[</span><span class="mi">34</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>  <span class="c1"># Subject 6
</span>    <span class="p">[</span><span class="mi">28</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>   <span class="c1"># Subject 7
</span><span class="p">])</span>

<span class="c1"># Define xi (Reference Subject, Subject 1)
</span><span class="n">xi</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Reference subject (Subject 1)
</span>
<span class="c1"># Define xj (Other Subjects)
</span><span class="n">xj</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>  <span class="c1"># Other subjects (Subjects 2 to 7)
</span>
<span class="c1"># Compute distances from Subject 1 to all other subjects
</span><span class="n">distances</span> <span class="o">=</span> <span class="p">[</span><span class="nf">mahalanobis_distance</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">xj</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span> <span class="k">for</span> <span class="n">xj</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>

<span class="c1"># Print results
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mahalanobis Distance d_M(1, </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">) = </span><span class="si">{</span><span class="n">d</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mahalanobis Distance d_M(1, 2) = 2.88
Mahalanobis Distance d_M(1, 3) = 0.23
Mahalanobis Distance d_M(1, 4) = 2.31
Mahalanobis Distance d_M(1, 5) = 2.00
Mahalanobis Distance d_M(1, 6) = 2.36
Mahalanobis Distance d_M(1, 7) = 3.03
</code></pre></div></div> <hr/> <p><br/></p> <h2 id="canberra-distance-weighted-manhattan"><strong>Canberra Distance (Weighted Manhattan)</strong></h2> <p>(Use for Text analysis or Gene Expression Data)</p> <p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs44147-024-00535-2/MediaObjects/44147_2024_535_Fig11_HTML.png?as=webp" alt="Canberra Distance" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"/></p> <h4 id="definition-7"><strong>Definition</strong></h4> <ul> <li> <p>For two d-dimensional vectors $\mathbf{s} = (s_1, s_2, …, s_d)$ and $\mathbf{t} = (t_1, t_2, …, t_d)$, the Canberra distance is defined as:</p> </li> <li> <p>Standard equation:</p> </li> </ul> \[d(\mathbf{s}, \mathbf{t}) = \sum_{j=1}^{d} \frac{|s_j - t_j|}{|s_j| + |t_j|}\] <ul> <li>Sometimes with variation, e.g. normalization:</li> </ul> \[d(\mathbf{s}, \mathbf{t}) = \frac{1}{d} \sum_{j=1}^{d} \frac{|s_j - t_j|}{|s_j| + |t_j|}\] <p>Sometimes a factor of 2 is seen in the formula (either in front or inside), but the core idea is that it’s a weighted version of Manhattan distance.</p> <h4 id="why-and-how-its-used-7"><strong>Why and How It’s Used</strong></h4> <ol> <li><strong>Weighted by Magnitude</strong>: <ul> <li>Each coordinate difference $\lvert s_j - t_j \rvert$ is divided by $\lvert s_j \rvert + \lvert t_j \rvert$.</li> <li>If both $\lvert s_j \rvert$ and $\lvert t_j \rvert$ are large, the difference is scaled down; if one value is small or zero, the difference is amplified.</li> </ul> </li> <li><strong>Very Sensitive to Small Values</strong>: <ul> <li>If a coordinate is near 0, then $\lvert s_j \rvert + \lvert t_j \rvert$ is very small, which causes that term’s distance value to become large.</li> <li>This makes Canberra distance very sensitive to changes in features with small or zero values.</li> </ul> </li> <li><strong>Application Scenarios</strong>: <ul> <li>Data such as text analysis or gene expression, where zero or near-zero counts are very important.</li> <li>Canberra distance can be used when we are more concerned with relative differences rather than absolute differences.</li> </ul> </li> </ol> <h4 id="how-it-works-intuitive-understanding-5"><strong>How It Works (Intuitive Understanding)</strong></h4> <ul> <li>For each coordinate j, first compute the absolute difference $\lvert s_j - t_j \rvert$.</li> <li>Then divide by the sum $\lvert s_j \rvert + \lvert t_j \rvert$. If this sum is small, then that coordinate’s contribution to the overall distance becomes larger.</li> <li>Sum the results for all dimensions to get the Canberra distance.</li> </ul> <p>Compute $\frac{|1 - 2|}{|1| + |2|} + \frac{|10 - 5|}{|10| + |5|} + \frac{|0 - 3|}{|0| + |3|}$. Each dimension, due to the different magnitudes of the denominators, contributes differently to the overall distance.</p> <h4 id="example-8"><strong>Example:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">canberra_distance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">s</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="c1"># To avoid division by zero, we can handle 0 in the denominator carefully:
</span>    <span class="c1"># We'll replace 0 with a small epsilon or skip that term if both s_j and t_j are 0.
</span>    <span class="c1"># Here, let's just do a safe division approach:
</span>    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-12</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="p">(</span><span class="n">denominator</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">ratio</span><span class="p">)</span>

<span class="c1"># Example usage:
</span><span class="n">point_a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">point_b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>

<span class="n">dist_canberra</span> <span class="o">=</span> <span class="nf">canberra_distance</span><span class="p">(</span><span class="n">point_a</span><span class="p">,</span> <span class="n">point_b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Canberra Distance:</span><span class="sh">"</span><span class="p">,</span> <span class="n">dist_canberra</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Canberra Distance: 1.6666666666661998
</code></pre></div></div> <p><br/></p> <hr/> <h2 id="final-note"><strong>Final Note:</strong></h2> <p>I believe it is very important to restate that all distances included in this blog post are the common ones that I used at least onece in either one of my school or work projects. So be aware that are many new distance measures that you may encounter in doing your own projects, don’t hesitate to learn.</p> <h4 id="many-distances-comparison"><strong>Many Distances Comparison</strong></h4> <p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UBVod31pjOcv41LJrBC7lg.jpeg" alt="Many Distances Comparison" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"/></p> <h2 id="image-reference"><strong>Image Reference</strong></h2> <ol> <li><a href="https://statistics.laerd.com/statistical-guides/img/spearman-1-small.png">Spearman Correlation</a></li> <li><a href="https://media.geeksforgeeks.org/wp-content/uploads/20250723175534566635/pearson_correlation_coefficient.webp">Pearson Correlation</a></li> <li><a href="https://media.geeksforgeeks.org/wp-content/uploads/20200911171455/UntitledDiagram2.png">Cosine Similarity</a></li> <li><a href="https://www.kdnuggets.com/wp-content/uploads/c_distance_metrics_euclidean_manhattan_minkowski_oh_12.jpg">Minkowski Distance</a></li> <li><a href="https://rosalind.info/media/Euclidean_distance.png">Euclidean Distance</a></li> <li><a href="https://iq.opengenus.org/content/images/2018/12/distance.jpg">Compare Three Distances</a></li> <li><a href="https://iq.opengenus.org/content/images/2018/12/chebyshev.png">Chebyshev Distance</a></li> <li><a href="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KzsugPQU-BTjvDACXbu9qw.jpeg">Mahalanobis Distance1</a></li> <li><a href="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UBVod31pjOcv41LJrBC7lg.jpeg">Many Distances Comparison</a></li> <li><a href="https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs44147-024-00535-2/MediaObjects/44147_2024_535_Fig11_HTML.png?as=webp">Canberra Distance</a></li> </ol>]]></content><author><name>Micheli Liu</name></author><category term="Stats"/><category term="Notes"/><summary type="html"><![CDATA[Include many distance measures: which come in handy and help me through many of my data science projects]]></summary></entry><entry><title type="html">Winning with Data: Goizueta MSBA Students Shine in Travelers University Modeling Competition - Voice of Goizueta</title><link href="https://micheliliuv87.github.io/blog/2025/winning-with-data-goizueta-msba-students-shine-in-travelers-university-modeling-competition-voice-of-goizueta/" rel="alternate" type="text/html" title="Winning with Data: Goizueta MSBA Students Shine in Travelers University Modeling Competition - Voice of Goizueta"/><published>2025-05-22T00:00:00+00:00</published><updated>2025-05-22T00:00:00+00:00</updated><id>https://micheliliuv87.github.io/blog/2025/winning-with-data-goizueta-msba-students-shine-in-travelers-university-modeling-competition---voice-of-goizueta</id><content type="html" xml:base="https://micheliliuv87.github.io/blog/2025/winning-with-data-goizueta-msba-students-shine-in-travelers-university-modeling-competition-voice-of-goizueta/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              Voice of Goizueta                
   by     
   Voice of Goizueta
 
 ·
                        
                            Published May 22, 2025
            · Updated May 22, 2025
                  Recently, two teams of MSBA students from Goizueta (Andy Dang 25MSBA, Chima Ewuzie 25MSBA, Eason Li 25MSBA, Micheli Liu 25MSBA, Evelyn Musembi 25MSBA, Ian Poe 25MSBA and Roffy Shan 25MSBA) showcased their analytical skills in the Travelers University Modeling Competition, an annual challenge hosted by Travelers Insurance.This prestigious competition tasks university students with developing predictive models using real-world business data, testing their ability to solve complex industry problems. Goizueta’s team not only rose to the challenge but stood out for their innovative approach, earning a spot in the semifinals. Here, we chat with one of the teams about their experience, key takeaways, and the strategies that set them apart.CloverShield Insurance seeks a predictive model to forecast policyholder call frequency, which would optimize call center resource allocation and reduce operational costs.Roffy, Micheli, and Eason are students from the MS in Business Analytics (MSBA) program at Goizuta Business School. Roffy completed his previous degree at Queen’s University and has had internships as a data analyst in many firms. Micheli graduated from Miami University with a finance degree, and previously worked at a security and venture capital firm as an equity analyst. Eason graduated from Zhongnan University with a degree in International Economics. He believes that the use of data is transforming the world.What inspired you to participate in the Travelers University Modeling competition, and how did you approach the competition?Roffy: We participated in the Travelers University Modeling Competition to explore the vivid world of data science in the insurance industry. Insurance is one of the first industries to have adopted sophisticated statistics and machine learning to add business value to its operations, and we were really eager to learn how the current trends are reflected in the actual day-to-day practice. The competition also provided the opportunity to learn from fellow competitors, many of whom are PhDs and experts with years of industry experience.For our model, we leveraged the predicting power of a carefully tuned advanced boosting algorithm and neural network to capture the complex nature of the data. We then built our prediction pipeline with a focus on understanding the distribution of the target variable and tuned the models accordingly.Could you share some insights into the experience of competing in the challenge and working with your faculty advisors?Micheli: We found the faculty’s feedback helpful when presenting our approach. They often questioned the reasoning behind our initial model and challenged us to evaluate whether our decisions were justified. Rather than providing direct answers, the professors encouraged us to think critically.What do you think contributed to your success in advancing to the semifinals? How did you stand out?Team: Our approach of building a stacked predictive pipeline, which includes a classification model to predict whether a customer will make a phone call and a regression model to predict the exact number of phone calls, was unique among the competitors. This innovation was particularly appealing to the judges.What advice would you give to future students considering participating in analytics-focused competitions like the Travelers University Modeling Challenge?Eason: We believe everyone should actively participate in these competitions because, in our future careers, we need to apply data to solve real-world problems rather than simply learning theoretical concepts from textbooks. Competitions like this provide an excellent platform to practice and build a solid foundation for our future work.How did the coursework you’ve completed in the program contribute to your success in the competition?Team: We applied all standard model-building methods taught for the competition. Throughout the process, we documented our assumptions, managed variables, and evaluated the model’s robustness. Our coursework in the MSBA program provided guidance on accessing resources and identifying requirements.Looking ahead, what are your future goals in the field of data analytics, and how do you plan to build on your experience from the Travelers University Modeling Competition to achieve them?Team: It is a good start to get to know more models that apply to various scenarios. It also reminds us to always keep track of what is advancing in the data analytics field, like new methods.Interested in exploring MS in Business Analytics opportunities? Learn more about Goizueta’s MS in Business Analytics program.Tags: case competitionMSBAVoice of GoizuetaVoice of Goizueta curates the student perspective at Emory University's Goizueta Business School.Follow:Undergraduate BBAThe Goizueta BBA helps students grow intellectually, personally, and professionally so that they feel enabled, empowered, and motivated to make significant positive contributions to the organizations they serve and to society as a whole.MS in Business AnalyticsGoizueta’s STEM-designated MS in Business Analytics combines business, data, and technology to make you an effective business data scientist for a data-driven world. This 10-month, immersive program emphasizes hands-on learning in real-world partnerships.Evening MBAFind the depth and breadth of learning you'd expect from a top-ranked MBA on a flexible schedule structured specifically for working professionals. Adjust your quantity, timing, and type of courses to match your professional goals and personal life.One-Year MBAThe top-ranked Goizueta One-Year MBA provides the full MBA experience—including world-class academics and professional and personal development within just 12 months—delivered in a dynamic, global city.Master in Business for VeteransCreated specifically for military veterans, active duty, National Guard, and Reserve personnel to seamlessly transition from military service to a career in business. Includes career development and mentoring programs specifically for veterans.Executive MBAExpand your breadth of knowledge and depth of expertise with the Emory Executive MBA. Designed for emerging executives and accomplished professionals, you'll tailor the EMBA to your professional interests, enhance your leadership abilities, and maximize your ability to create organizational value.Two-Year MBAThe only top-ranked MBA offering world-class academics and small-by-design classes delivered in a dynamic, global city. Receive high-quality, personalized feedback plus access to Fortune 500s and emerging businesses alike.Master of FinanceCreated by finance industry insiders to develop next gen finance professionals, this 10-month, STEM-designated finance master's degree delivers unmatched experience and industry know how through our unique analyst immersion and Finance Lab.Executive EducationEmory Executive Education understands marketplace trends and delivers programming focused on the most current challenges in business. With short courses and custom program options, individuals and organizations have an academic talent development partner.Copyright Emory University © 2025 - All Rights Reserved   Goizueta Business School, 1300 Clifton Road, Atlanta, Georgia 30322 
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Goizueta MSBA students excelled in the Travelers University Modeling Competition, advancing to the semifinals with their innovative predictive model. Read about their experience and key takeaways.]]></summary></entry><entry><title type="html">Autoencoder Basics and How to Implement</title><link href="https://micheliliuv87.github.io/blog/2025/distance-measure-copy/" rel="alternate" type="text/html" title="Autoencoder Basics and How to Implement"/><published>2025-05-14T14:36:12+00:00</published><updated>2025-05-14T14:36:12+00:00</updated><id>https://micheliliuv87.github.io/blog/2025/distance-measure%20copy</id><content type="html" xml:base="https://micheliliuv87.github.io/blog/2025/distance-measure-copy/"><![CDATA[<h1 id="still-updating">(Still Updating)</h1> <h2 id="sparse-autoencoder-sae"><strong>Sparse Autoencoder (SAE)</strong></h2> <h3 id="definition"><strong>Definition:</strong></h3> <p>A sparse autoencoder is a type of neural network used to learn compact, interpretable representations of data by enforcing sparsity in its hidden layer activations. Unlike regular autoencoders that mainly compress and reconstruct data, sparse autoencoders add a sparsity penalty (such as L1 regularization or KL divergence) to encourage only a small subset of neurons to be activated at any given time, leading to extraction of the most salient features and helping prevent overfitting. (Check out very interesting <a href="https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf">lecture notes</a> by Andrew Ng)</p> <h3 id="understanding-in-simple-words--implementation"><strong>Understanding in Simple Words &amp; Implementation</strong></h3> <ul> <li> <p>“A SAE enforces sparsity in the hidden layer activations”</p> <ul> <li>Regular autoencoders minimize reconstruction error only.</li> <li>Sparse autoencoders add a constraint: they penalize the model when too many hidden units are active.</li> <li>This pushes the model to keep most neuron activations near zero.</li> </ul> </li> <li> <p>“The idea is to make most of the neurons inactive, forcing the model to learn efficient feature representations.”</p> <ul> <li>Focus on a few features at a time,</li> <li>Learn more interpretable and robust representations,</li> <li>Avoid overfitting or trivial identity mappings.</li> </ul> </li> <li> <p><strong>Loss Function</strong>: \(\text{L} = \sum_{i=1}^{n} \|X_i - X_i{\prime}\|^2 + \lambda \sum |z_i|\)</p> </li> </ul> <p><strong>This loss function does two things:</strong></p> <ol> <li> <p>Minimizes reconstruction error so that the autoencoder can accurately represent the input.</p> </li> <li> <p>Adds an L1 penalty on the hidden activations z_i, encouraging sparse activations.</p> </li> </ol> <p>This is similar to how Lasso regression enforces sparsity in coefficients.</p> <p><br/></p> <table> <thead> <tr> <th>Symbol</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>$X_i$</td> <td>The original input sample</td> </tr> <tr> <td>$X_i’$</td> <td>The reconstructed output (decoded from latent representation)</td> </tr> <tr> <td>$|X_i - X_i’|^2$</td> <td>Reconstruction loss: how close the output is to the input</td> </tr> <tr> <td>$z_i$</td> <td>Hidden layer activation for input ( i )</td> </tr> <tr> <td>$\sum \lvert z_i \rvert$</td> <td>Sum of absolute hidden activations — the sparsity penalty</td> </tr> <tr> <td>$\lambda$</td> <td>Regularization parameter controlling the strength of sparsity</td> </tr> </tbody> </table> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="c1"># 1) Define the Sparse Autoencoder
</span><span class="k">class</span> <span class="nc">SparseAutoencoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># encoder: input → hidden
</span>        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="c1"># decoder: hidden → reconstruction
</span>        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="c1"># encode with sigmoid activation
</span>        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="c1"># decode with sigmoid (or no activation, depending on your data)
</span>        <span class="n">x_recon</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x_recon</span><span class="p">,</span> <span class="n">z</span>

<span class="c1"># 2) Hyperparameters
</span><span class="n">input_dim</span>    <span class="o">=</span> <span class="mi">784</span>   <span class="c1"># e.g. flattened 28×28 image
</span><span class="n">hidden_dim</span>   <span class="o">=</span> <span class="mi">64</span>    <span class="c1"># size of latent (bottleneck)
</span><span class="n">lambda_sparse</span> <span class="o">=</span> <span class="mf">1e-3</span> <span class="c1"># weight of the sparsity penalty
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>

<span class="c1"># 3) Instantiate model, loss &amp; optimizer
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">SparseAutoencoder</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
<span class="n">mse_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># 4) Dummy data loader (replace with real dataset)
#    Here: 100 samples of dimension 784
</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>

<span class="c1"># 5) Training loop (one epoch for illustration)
</span><span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>            <span class="c1"># if you have a real DataLoader, iterate batches
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>    <span class="c1"># make it shape (1, 784)
</span>    
    <span class="c1"># Forward pass
</span>    <span class="n">x_recon</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># Reconstruction loss
</span>    <span class="n">recon_loss</span> <span class="o">=</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="n">x_recon</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># Sparsity loss: L1 on hidden activations
</span>    <span class="c1"># Sum over hidden dims, mean over batch
</span>    <span class="n">sparsity_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    
    <span class="c1"># Total loss
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">recon_loss</span> <span class="o">+</span> <span class="n">lambda_sparse</span> <span class="o">*</span> <span class="n">sparsity_loss</span>
    
    <span class="c1"># Backpropagation
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Recon: </span><span class="si">{</span><span class="n">recon_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">  |  Sparse: </span><span class="si">{</span><span class="n">sparsity_loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">  |  Total: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-md highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Recon: 1.2073  |  Sparse: 0.5127  |  Total: 1.2078
Recon: 1.3084  |  Sparse: 0.5057  |  Total: 1.3089
Recon: 1.1943  |  Sparse: 0.5150  |  Total: 1.1949
Recon: 1.3257  |  Sparse: 0.4914  |  Total: 1.3262
Recon: 1.1187  |  Sparse: 0.4987  |  Total: 1.1192
Recon: 1.0752  |  Sparse: 0.4697  |  Total: 1.0756
Recon: 1.1239  |  Sparse: 0.5137  |  Total: 1.1244
...
</code></pre></div></div> <hr/> <h2 id="references"><strong>References</strong></h2> <ol> <li><a href="https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf">Sparse Autoencoder Lecture Notes by Andrew Ng</a></li> </ol>]]></content><author><name>Micheli Liu</name></author><category term="Neural"/><category term="Network"/><category term="Notes"/><summary type="html"><![CDATA[Simple implementation of autoencoder framework]]></summary></entry><entry><title type="html">Practical Applications of the Bag of Words Model</title><link href="https://micheliliuv87.github.io/blog/2025/bag-of-words-2/" rel="alternate" type="text/html" title="Practical Applications of the Bag of Words Model"/><published>2025-01-05T21:30:21+00:00</published><updated>2025-01-05T21:30:21+00:00</updated><id>https://micheliliuv87.github.io/blog/2025/bag-of-words-2</id><content type="html" xml:base="https://micheliliuv87.github.io/blog/2025/bag-of-words-2/"><![CDATA[<h2 id="introduction--recap"><strong>Introduction &amp; Recap</strong></h2> <p>In our previous post, we explored the <strong>theoretical foundations</strong> of the Bag of Words (BoW) model—how it converts unstructured text into numerical representations by counting word frequencies while disregarding word order and grammar. This fundamental technique has proven remarkably effective across various <strong>Natural Language Processing (NLP) tasks</strong> despite its simplicity .</p> <p>Now, let’s transition from theory to practice. In this hands-on guide, we’ll implement the Bag of Words model for several real-world applications including text classification, sentiment analysis, and TF-IDF visualization. We’ll work with Python libraries like <code class="language-plaintext highlighter-rouge">scikit-learn</code>, <code class="language-plaintext highlighter-rouge">NLTK</code>, and <code class="language-plaintext highlighter-rouge">matplotlib</code> to build working examples you can adapt for your own projects.</p> <h2 id="text-classification-with-bow-spam-detection-example"><strong>Text Classification with BoW: Spam Detection Example</strong></h2> <p>One of the most common applications of Bag of Words is <strong>text classification</strong>, where we categorize documents into predefined classes. Let’s build a spam detection system that classifies messages as “spam” or “not spam.”</p> <h4 id="step-by-step-implementation"><strong>Step-by-Step Implementation</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">re</span>
<span class="kn">import</span> <span class="n">nltk</span>
<span class="kn">from</span> <span class="n">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="n">nltk.stem</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>
<span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># download NLTK sample data
</span><span class="n">nltk</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="sh">'</span><span class="s">stopwords</span><span class="sh">'</span><span class="p">)</span>
<span class="n">nltk</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="sh">'</span><span class="s">punkt</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># sample dataset (in practice, you'd use a larger dataset)
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">Win a free iPhone now! Click here to claim your prize.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Your package has been delivered. Track your shipment.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Congratulations! You won a $1000 gift card. Reply to claim.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Meeting scheduled for tomorrow at 10 AM in conference room.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Urgent! Your account needs verification. Update immediately.</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">The quarterly report is attached for your review.</span><span class="sh">'</span>
    <span class="p">],</span>
    <span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">not spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">not spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">spam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">not spam</span><span class="sh">'</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># text preprocessing function
</span><span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># remove non-alphabetic characters and convert to lowercase
</span>    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">[^A-Za-z]</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">())</span>

    <span class="c1">#tokenize
</span>    <span class="n">words</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="c1">#remove stopwords
</span>    <span class="n">stop_words</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">))</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>

    <span class="c1">#apply stemming
</span>    <span class="n">stemmer</span> <span class="o">=</span> <span class="nc">PorterStemmer</span><span class="p">()</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="p">.</span><span class="nf">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>

    <span class="k">return</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="c1"># apply preprocessing
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">processed_text</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span><span class="n">preprocess_text</span><span class="p">)</span>

<span class="c1"># create Bag of Words representation
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">processed_text</span><span class="sh">'</span><span class="p">]).</span><span class="nf">toarray</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># split data into training and testing sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># train a Naive Bayes classifier
</span><span class="n">classifier</span> <span class="o">=</span> <span class="nc">GaussianNB</span><span class="p">()</span>
<span class="n">classifier</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># make predictions and evaluate accuracy
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Model Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h4 id="how-bow-helps-in-classification"><strong>How BoW Helps in Classification</strong></h4> <p>In this spam detection example, the Bag of Words model identifies <strong>characteristic word patterns</strong> in spam versus legitimate messages . Spam messages often contain words like “win,” “free,” “prize,” and “urgent,” while legitimate messages use more neutral language. By converting these word patterns into numerical features, we enable machine learning algorithms to learn the distinguishing characteristics of each category.</p> <p>The <code class="language-plaintext highlighter-rouge">CountVectorizer</code> from scikit-learn handles the heavy lifting of creating our BoW representation . The <code class="language-plaintext highlighter-rouge">max_features</code> parameter ensures we only consider the top 1000 most frequent words, preventing excessively high-dimensional data.</p> <h2 id="sentiment-analysis-with-bow"><strong>Sentiment Analysis with BoW</strong></h2> <p>Another powerful application of Bag of Words is <strong>sentiment analysis</strong>—determining whether a piece of text expresses positive, negative, or neutral sentiment. Let’s analyze movie reviews.</p> <h4 id="implementation-code"><strong>Implementation Code</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Sample movie reviews dataset
</span><span class="n">reviews</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">This movie was absolutely fantastic! Great acting and plot.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Terrible waste of time. Poor acting and boring story.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Loved the cinematography and character development.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">The worst movie I have ever seen in my life.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Brilliant performance by the lead actor. Highly recommended.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Mediocre at best. Nothing special about this film.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">An outstanding masterpiece that kept me engaged throughout.</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Poor direction and weak screenplay. Very disappointing.</span><span class="sh">'</span>
<span class="p">]</span>

<span class="n">sentiments</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">positive</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">positive</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span><span class="p">,</span>
              <span class="sh">'</span><span class="s">positive</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">positive</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">negative</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># Create BoW model with unigrams and bigrams
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stop_words</span><span class="o">=</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">reviews</span><span class="p">)</span>

<span class="c1"># Train a classifier
</span><span class="n">classifier</span> <span class="o">=</span> <span class="nc">MultinomialNB</span><span class="p">()</span>
<span class="n">classifier</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">sentiments</span><span class="p">)</span>

<span class="c1"># Test with new reviews
</span><span class="n">test_reviews</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">The acting was good but the story was weak</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Amazing movie with fantastic performances</span><span class="sh">'</span>
<span class="p">]</span>

<span class="n">test_vectors</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">test_reviews</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">test_vectors</span><span class="p">)</span>

<span class="k">for</span> <span class="n">review</span><span class="p">,</span> <span class="n">sentiment</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">test_reviews</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Review: </span><span class="sh">'</span><span class="si">{</span><span class="n">review</span><span class="si">}</span><span class="sh">'</span><span class="s"> -&gt; Sentiment: </span><span class="si">{</span><span class="n">sentiment</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h4 id="understanding-the-output"><strong>Understanding the Output</strong></h4> <p>This example introduces an important enhancement: <strong>ngrams</strong> . By setting <code class="language-plaintext highlighter-rouge">ngram_range=(1, 2)</code>, we consider both single words (unigrams) and pairs of consecutive words (bigrams). This helps capture phrases like “absolutely fantastic” or “poor acting” that carry more nuanced sentiment than individual words.</p> <p>The <code class="language-plaintext highlighter-rouge">MultinomialNB</code> classifier is particularly well-suited for text classification with discrete features like word counts . It efficiently learns the probability distributions of words for each sentiment class.</p> <h2 id="tf-idf-enhancing-bag-of-words"><strong>TF-IDF: Enhancing Bag of Words</strong></h2> <p>While simple word counts are useful, they have a limitation: common words that appear frequently across all documents may dominate the analysis. <strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong> addresses this by weighting words based on their importance .</p> <h4 id="tf-idf-calculation-and-visualization"><strong>TF-IDF Calculation and Visualization</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># sample documents for TF-IDF demonstration
</span><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">Natural language processing with Python is fascinating and powerful</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Python is a great programming language for data science</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">I enjoy learning new things about NLP and Python programming</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Machine learning and artificial intelligence are changing the world</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Deep learning models require extensive computational resources</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Data scientists use Python for machine learning projects</span><span class="sh">"</span>
<span class="p">]</span>

<span class="c1"># calculate TF-IDF
</span><span class="n">tfidf_vectorizer</span> <span class="o">=</span> <span class="nc">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">)</span>
<span class="n">tfidf_matrix</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

<span class="c1"># get feature names and TF-IDF scores
</span><span class="n">feature_names</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">()</span>
<span class="n">dense_matrix</span> <span class="o">=</span> <span class="n">tfidf_matrix</span><span class="p">.</span><span class="nf">todense</span><span class="p">()</span>

<span class="c1"># display TF-IDF scores for first document
</span><span class="n">first_doc</span> <span class="o">=</span> <span class="n">dense_matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">word_scores</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">first_doc</span><span class="p">),</span>
                    <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Top terms in first document:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">word_scores</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1">#dimension reduction for visualization
</span><span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca_result</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">dense_matrix</span><span class="p">)</span>

<span class="c1"># create visualization
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">pca_result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">pca_result</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>

<span class="c1">#ddd document labels
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">txt</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([</span><span class="sa">f</span><span class="sh">"</span><span class="s">Doc </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">documents</span><span class="p">))]):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">pca_result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">pca_result</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                 <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="sh">'</span><span class="s">offset points</span><span class="sh">'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">TF-IDF Document Visualization using PCA</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Principal Component 1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Principal Component 2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h4 id="tf-idf-intuition"><strong>TF-IDF Intuition</strong></h4> <p>TF-IDF balances two factors :</p> <ul> <li><strong>Term Frequency (TF)</strong>: How often a word appears in a specific document</li> <li><strong>Inverse Document Frequency (IDF)</strong>: How rare the word is across all documents</li> </ul> <p>The product of these two values gives higher weight to words that are frequent in a specific document but rare in the overall collection. This effectively identifies words that are characteristic of each document.</p> <p><img src="https://i.ytimg.com/vi/zLMEnNbdh4Q/maxresdefault.jpg" alt="TF-IDF Visualization" style="display:block; margin:0 auto; max-width:100%; height:auto; max-height:480px;"/></p> <p><em>TF-IDF helps identify the most distinctive words in documents, pushing similar documents closer together in vector space .</em></p> <h2 id="advanced-techniques--best-practices"><strong>Advanced Techniques &amp; Best Practices</strong></h2> <h4 id="feature-engineering-with-n-grams"><strong>Feature Engineering with N-grams</strong></h4> <p>As mentioned in the sentiment analysis example, n-grams can significantly improve model performance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># comparing different n-gram ranges
</span><span class="n">unigram_vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">bigram_vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">trigram_vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># each approach captures different linguistic patterns
</span></code></pre></div></div> <p>Bigrams and trigrams help preserve contextual information that single words might lose . For example, the phrase “not good” has a very different meaning than the individual words “not” and “good” considered separately.</p> <h4 id="handling-large-vocabularies-with-feature-hashing"><strong>Handling Large Vocabularies with Feature Hashing</strong></h4> <p>For extremely large datasets, the BoW representation can become computationally challenging. <strong>Feature hashing</strong> (or the “hash trick”) provides a memory-efficient alternative :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">HashingVectorizer</span>

<span class="c1"># using hashing vectorizer for large datasets
</span><span class="n">hash_vectorizer</span> <span class="o">=</span> <span class="nc">HashingVectorizer</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">alternate_sign</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">X_hash</span> <span class="o">=</span> <span class="n">hash_vectorizer</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre></div></div> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>The Bag of Words model, while simple, remains remarkably powerful for practical text analysis tasks. Through our implementations, we’ve seen how BoW enables:</p> <ul> <li><strong>Text classification</strong> by identifying characteristic word patterns in different categories</li> <li><strong>Sentiment analysis</strong> by capturing emotional language through word frequencies</li> <li><strong>Enhanced analysis with TF-IDF</strong> by weighting words based on their distinctiveness</li> </ul> <p>The true power of Bag of Words lies in its <strong>versatility and interpretability</strong>. Unlike more complex deep learning models, BoW features are easily understandable and can provide valuable insights into what the model is learning.</p> <p>While modern approaches like word embeddings and transformer models have their place for complex NLP tasks, Bag of Words remains an excellent starting point for most text analysis projects—offering a compelling balance of simplicity, interpretability, and effectiveness.</p> <p><em>All code examples in this post are designed to be runnable with Python 3.6+ and standard data science libraries (pandas, scikit-learn, NLTK, matplotlib).</em></p> <hr/> <p><em>This practical guide builds upon the theoretical foundations established in our previous post about the Bag of Words model. Implement these techniques as a starting point for your text analysis projects, then experiment with different preprocessing approaches and parameter tuning to optimize for your specific use cases.</em></p> <h2 id="references"><strong>References</strong>:</h2> <ol> <li><a href="https://www.datacamp.com/tutorial/python-bag-of-words-model">Python Bag of Words Models</a></li> <li><a href="https://www.geeksforgeeks.org/machine-learning/visualizing-tf-idf-scores-a-comprehensive-guide-to-plotting-a-document-tf-idf-2d-graph/">Visualizing TF-IDF Scores: A Comprehensive Guide to Plotting a Document TF-IDF 2D Graph</a></li> <li><a href="https://medium.com/swlh/text-classification-using-the-bag-of-words-approach-with-nltk-and-scikit-learn-9a731e5c4e2f">Text classification using the Bag Of Words Approach with NLTK and Scikit Learn</a></li> <li><a href="https://medium.com/@GeoffreyGordonAshbrook/vector-visualization-2d-plot-your-tf-idf-with-pca-83fa9fccb1d">Vector Visualization: 2D Plot your TF-IDF with PCA</a></li> <li><a href="https://www.geeksforgeeks.org/nlp/bag-of-words-bow-model-in-nlp/">Bag of words (BoW) model in NLP</a></li> <li><a href="https://www.analyticsvidhya.com/blog/2021/08/a-friendly-guide-to-nlp-bag-of-words-with-python-example/">A friendly guide to NLP: Bag-of-Words with Python example</a></li> <li><a href="https://www.tidytextmining.com/tfidf">3 Analyzing word and document frequency: tf-idf</a></li> <li><a href="https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf">Analyzing Documents with TF-IDF</a></li> <li><a href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/">A Gentle Introduction to the Bag-of-Words Model</a></li> <li><a href="https://blog.quantinsti.com/bag-of-words/">Bag of Words: Approach, Python Code, Limitations</a></li> </ol>]]></content><author><name>Emory University ISOM</name></author><category term="Text"/><category term="Processing"/><category term="Tutorials"/><summary type="html"><![CDATA[Application of Bag of words, examples of TF-IDF, Text Classification, and Sentiment Analysis]]></summary></entry><entry><title type="html">The Bag of Words Model, A Comprehensive Analysis of NLP’s Foundational Technique</title><link href="https://micheliliuv87.github.io/blog/2025/bag-of-words-1/" rel="alternate" type="text/html" title="The Bag of Words Model, A Comprehensive Analysis of NLP’s Foundational Technique"/><published>2025-01-02T16:40:16+00:00</published><updated>2025-01-02T16:40:16+00:00</updated><id>https://micheliliuv87.github.io/blog/2025/bag-of-words-1</id><content type="html" xml:base="https://micheliliuv87.github.io/blog/2025/bag-of-words-1/"><![CDATA[<h2 id="introduction-to-bow"><strong>Introduction to BoW</strong></h2> <p>In the landscape of <strong>Natural Language Processing</strong> (NLP), few models have been as fundamentally important and enduringly influential as the <strong>Bag of Words</strong> (<a href="https://en.wikipedia.org/wiki/Bag-of-words_model">BoW</a>) model. At its core, BoW represents a straightforward yet powerful approach to text representation: it transforms unstructured text into a structured, numerical format by treating a document as an unordered collection of words while tracking their frequency. This conceptual simplicity has made BoW a <strong>foundational technique</strong> that continues to serve as a baseline for text classification and feature extraction tasks, despite the emergence of more sophisticated alternatives.</p> <p>The BoW model operates on a fundamental assumption that the frequency of words in a document captures meaningful information about its content, regardless of their order or grammatical relationships . This approach might seem counterintuitive—after all, human language relies heavily on word order and syntax for meaning—yet BoW has proven remarkably effective for many <a href="https://builtin.com/machine-learning/bag-of-words">practical NLP applications</a>, from spam detection to sentiment analysis.</p> <h2 id="history-in-short"><strong>History in Short</strong></h2> <p>The conceptual origins of Bag of Words trace back to the mid-20th century, with early references found in Zellig Harris’s 1954 article on “Distributional Structure” . The model emerged from the intersection of <strong>computational linguistics</strong> and <strong>information retrieval</strong> during the 1950s, when researchers sought pragmatic solutions for processing text data without needing to understand complex grammatical structures.</p> <p>Initially developed in the context of document classification and early information retrieval systems, BoW gained significant traction as researchers recognized that <strong>word frequency patterns</strong> could provide substantial insights into document content and categorization . By the 1990s, BoW had become a standard technique in natural language processing, finding robust applications in spam filtering, document classification, and early sentiment analysis systems.</p> <p>The mathematical foundation of BoW—representing documents as vectors where each dimension corresponds to a unique word and the value represents its frequency—revolutionized text analysis by enabling computational systems to perform mathematical operations on textual data . This transformation from unstructured text to structured numerical representation opened new possibilities for machine learning applications in natural language.</p> <h2 id="how-bag-of-words-works"><strong>How Bag of Words Works?</strong></h2> <h4 id="central-mechanism"><strong>Central Mechanism</strong></h4> <p>The Bag of Words model transforms text through a systematic process that disregards word order and syntax while preserving information about word occurrence and frequency . The standard implementation involves three key steps:</p> <ol> <li><strong>Tokenization</strong>: Breaking down text into individual words or tokens</li> <li><strong>Vocabulary Creation</strong>: Building a unique dictionary of all distinct words across the corpus</li> <li><strong>Vectorization</strong>: Converting each document into a numerical vector based on word frequencies</li> </ol> <h4 id="practical-implementation"><strong>Practical Implementation</strong></h4> <p>Here’s a concrete example that illustrates the BoW process:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="c1"># sample documents
</span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">I love programming and programming is fun</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">I love machine learning</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Machine learning is fascinating</span><span class="sh">'</span>
<span class="p">]</span>

<span class="c1"># build and fit vectorizer
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="nc">CountVectorizer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="c1"># print
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Vocabulary:</span><span class="sh">"</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">BoW matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="nf">toarray</span><span class="p">())</span>
</code></pre></div></div> <p>You would expect the following output:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Vocabulary: ['and' 'fascinating' 'fun' 'is' 'learning' 'love' 'machine' 'programming']
BoW matrix:
 [[1 0 1 1 0 1 0 2]
 [0 0 0 0 1 1 1 0]
 [0 1 0 1 1 0 1 0]]
</code></pre></div></div> <p>The resulting matrix represents each document as a vector where each element corresponds to the frequency of a specific word from the vocabulary . The first document, for instance, contains the word “and” once, “fun” once, “is” once, “love” once, and “programming” twice.</p> <p>The BoW model creates what can be described as a <strong>vector space</strong> where each unique word becomes a separate dimension . Documents are then plotted as points in this multi-dimensional space, with their positions along each dimension determined by the frequency of the corresponding word. This representation enables mathematical comparison and analysis of documents based on their word distribution patterns.</p> <h2 id="applications-and-use-cases"><strong>Applications and Use Cases</strong></h2> <p>The Bag of Words model has found diverse applications across numerous domains of text analysis and machine learning:</p> <h4 id="text-classification"><strong>Text Classification</strong></h4> <p>BoW serves as a fundamental feature extraction technique for <strong>document categorization</strong> tasks. Email services extensively use BoW for spam detection by analyzing the frequency of specific words indicative of spam content . Similarly, news organizations employ BoW for <strong>topic classification</strong>, automatically categorizing articles based on their predominant vocabulary .</p> <h4 id="sentiment-analysis"><strong>Sentiment Analysis</strong></h4> <p>Companies leverage BoW to understand <strong>customer sentiment</strong> across reviews, social media, and feedback platforms. By mapping word frequencies to positive or negative sentiment indicators, businesses can gauge public opinion about products or services at scale . For instance, words like “awful” or “terrible” appearing frequently in product reviews strongly indicate negative sentiment, while “excellent” or “amazing” suggest positive experiences .</p> <h4 id="information-retrieval"><strong>Information Retrieval</strong></h4> <p>Early search engines relied heavily on BoW principles to match user queries with relevant documents . While modern search algorithms have incorporated more sophisticated techniques, the fundamental approach of measuring <strong>word frequency</strong> and <strong>presence</strong> remains crucial in information retrieval systems.</p> <h4 id="other-applications"><strong>Other Applications</strong></h4> <ul> <li><strong>Document similarity detection</strong>: Identifying similar documents based on shared word distribution patterns</li> <li><strong>Language identification</strong>: Determining the language of a document based on characteristic vocabulary</li> <li><strong>Recommendation systems</strong>: Analyzing product descriptions or user reviews to generate personalized recommendations</li> <li><strong>Text clustering</strong>: Grouping similar documents together without predefined categories</li> </ul> <h2 id="limitations-and-challenges"><strong>Limitations and Challenges</strong></h2> <p>Despite its widespread adoption and utility, the Bag of Words model faces several significant limitations:</p> <h4 id="contextual-understanding"><strong>Contextual Understanding</strong></h4> <p>The most notable drawback of BoW is its <strong>complete disregard for word order</strong> and contextual relationships . This limitation means that sentences with identical words but different meanings receive identical representations. For example, “Man bites dog” and “Dog bites man” are treated as the same by BoW, despite their dramatically different meanings . Similarly, BoW cannot distinguish between “I am happy” and “I am not happy” since it doesn’t capture negations or syntactic relationships .</p> <h4 id="semantic-limitations"><strong>Semantic Limitations</strong></h4> <p>BoW operates at a superficial lexical level without capturing deeper semantic relationships:</p> <ul> <li><strong>Polysemy</strong>: Words with multiple meanings (like “bat” as a sports equipment or animal) are collapsed into a single representation</li> <li><strong>Synonymy</strong>: Different words with similar meanings (like “scary” and “frightening”) are treated as completely distinct features</li> <li><strong>Conceptual phrases</strong>: Multi-word expressions that form single semantic units (like “New York” or “artificial intelligence”) are broken down into individual components, losing their unified meaning</li> </ul> <h4 id="computational-considerations"><strong>Computational Considerations</strong></h4> <p>As vocabulary size increases, BoW vectors become <strong>high-dimensional</strong> and <strong>sparse</strong> (containing mostly zeros) . This sparsity can lead to computational inefficiency and the “curse of dimensionality” in machine learning models. For large datasets with extensive vocabularies, the resulting BoW representation may require significant memory and processing resources .</p> <h2 id="evolution-and-alternatives"><strong>Evolution and Alternatives</strong></h2> <h4 id="tf-idf-addressing-word-importance"><strong>TF-IDF: Addressing Word Importance</strong></h4> <p><strong>Term Frequency-Inverse Document Frequency</strong> (TF-IDF) emerged as an enhancement to basic BoW by addressing its limitation of treating all words equally . TF-IDF adjusts word weights by considering both:</p> <ul> <li><strong>Term Frequency</strong>: How often a word appears in a specific document</li> <li><strong>Inverse Document Frequency</strong>: How rare the word is across the entire document collection</li> </ul> <p>This approach reduces the influence of common words that appear frequently across many documents while emphasizing words that are distinctive to particular documents . The comparison below highlights key differences:</p> <hr/> <table> <thead> <tr> <th>Aspect</th> <th>Bag-of-Words (BoW)</th> <th>TF-IDF</th> </tr> </thead> <tbody> <tr> <td><strong>Word Importance</strong></td> <td>Treats all words equally</td> <td>Adjusts importance based on rarity</td> </tr> <tr> <td><strong>Handling Common Words</strong></td> <td>Common words can dominate representation</td> <td>Reduces weight of common words</td> </tr> <tr> <td><strong>Document Length Sensitivity</strong></td> <td>Highly sensitive to document length</td> <td>Normalizes for document length</td> </tr> <tr> <td><strong>Complexity</strong></td> <td>Simple and computationally inexpensive</td> <td>More complex due to IDF calculation</td> </tr> </tbody> </table> <hr/> <h4 id="word-embeddings-and-deep-learning-approaches"><strong>Word Embeddings and Deep Learning Approaches</strong></h4> <p>More advanced techniques have emerged to address BoW’s limitations:</p> <ul> <li><strong>Word2Vec</strong> (2013): Creates dense vector representations that capture semantic relationships between words based on their contextual usage</li> <li><strong>GloVe</strong> (Global Vectors): Uses global word co-occurrence statistics to generate word embeddings</li> <li><strong>FastText</strong>: Extends Word2Vec by representing words as bags of character n-grams, effectively handling out-of-vocabulary words</li> </ul> <h4 id="modern-transformer-models"><strong>Modern Transformer Models</strong></h4> <p>The field has evolved toward increasingly sophisticated architectures:</p> <ul> <li><strong>BERT</strong> (2018): Bidirectional Transformer models that capture contextual word meanings based on surrounding text</li> <li><strong>GPT series</strong>: Autoregressive models that generate human-like text by predicting subsequent words</li> <li><strong>RoBERTa, T5, and others</strong>: Optimized variants that improve upon earlier transformer architectures</li> </ul> <p>These modern approaches represent a paradigm shift from the context-agnostic nature of BoW to models that capture rich contextual and semantic relationships .</p> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p>Despite its simplicity and limitations, the Bag of Words model maintains <strong>enduring relevance</strong> in the NLP landscape. Its computational efficiency, interpretability, and effectiveness for specific tasks ensure its continued utility, particularly for:</p> <ul> <li><strong>Baseline models</strong>: Providing a performance benchmark for more complex algorithms</li> <li><strong>Resource-constrained environments</strong>: Offering a lightweight solution when computational resources are limited</li> <li><strong>Specific applications</strong>: Remaining effective for tasks where word presence alone provides strong signals, such as spam detection and topic classification</li> </ul> <p>The evolution from Bag of Words to modern transformer models illustrates the iterative nature of technological progress in natural language processing. While contemporary approaches have undoubtedly surpassed BoW in capturing linguistic nuance and context, they build upon the fundamental intuition behind BoW: that statistical patterns of word distribution contain meaningful information about document content .</p> <p>As we continue to develop increasingly sophisticated language models, the Bag of Words approach remains a critical milestone in our understanding of how machines can process and analyze human language. Its legacy endures not only in specific applications but in the foundational principles it established for text representation in computational systems.</p> <h2 id="references"><strong>References</strong></h2> <ol> <li><a href="https://en.wikipedia.org/wiki/Bag-of-words_model">Bag-of-words model</a></li> <li><a href="https://builtin.com/machine-learning/bag-of-words">Bag-of-Words Model in NLP Explained</a></li> <li><a href="https://www.geeksforgeeks.org/nlp/bag-of-words-vs-tf-idf/">Bag-of-words vs TF-IDF</a></li> <li><a href="https://aiml.com/what-are-some-use-cases-of-bag-of-words-model/">Use cases of Bag of Words model</a></li> <li><a href="https://www.byteplus.com/en/topic/400438">The Origins of Bag of Words</a></li> <li><a href="https://www.geeksforgeeks.org/nlp/bag-of-words-bow-model-in-nlp/">Bag of words (BoW) model in NLP</a></li> <li><a href="https://medium.com/@nagvekar/bag-of-words-simplified-a-hands-on-guide-with-code-advantages-and-limitations-in-nlp-f47461873068">Medium: Bag of Words Simplified</a></li> <li><a href="https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/">Introduction to Bag-of-Words and TF-IDF</a></li> <li><a href="https://www.ibm.com/think/topics/bag-of-words">What is bag of words?</a></li> <li><a href="https://medium.com/nlplanet/a-brief-timeline-of-nlp-from-bag-of-words-to-the-transformer-family-7caad8bbba56">A Brief Timeline of NLP</a></li> </ol>]]></content><author><name>Emory University ISOM</name></author><category term="Text"/><category term="Processing"/><category term="Tutorials"/><summary type="html"><![CDATA[BoW introduction, history, and use cases]]></summary></entry></feed>